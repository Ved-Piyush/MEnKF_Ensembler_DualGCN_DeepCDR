{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa53e19a-64db-4b1d-9378-c19c8efacd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 17:40:08.945339: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-10 17:40:08.948186: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-10 17:40:08.988061: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-10 17:40:08.988788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-10 17:40:09.986941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import block_diag\n",
    "import warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6028973-9799-4d8a-b919-a8dd6283faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34242ac3-a983-4bab-b092-bfcea4f94a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(weights_ann_1[0].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d459b853-f520-4181-a9a9-310a6dd20de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, initial_ensembles, size_ens): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    # weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    # h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a66ee4c-4112-4dca-a9cc-68a60d4e316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 32, input_shape = 256, output_shape = 1): \n",
    "    input_layer = tf.keras.layers.Input(shape = (input_shape))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(output_shape, activation = \"relu\")\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b9ef03-af61-4ce4-99ff-2f0b5dca639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_ensembles(num_weights, lambda1, size_ens):\n",
    "    mean_vec = np.zeros((num_weights,))\n",
    "    cov_matrix = lambda1*np.identity(num_weights)\n",
    "    mvn_samp = mvn(mean_vec, cov_matrix)\n",
    "    return mvn_samp.rvs(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2ea2b5-13a4-41c4-8257-c6c18708501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "758ac30a-bcc7-4b5e-9314-7fb85f284f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann =  ann(hidden = 16, input_shape = 32, output_shape = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2548f2de-a4c3-4850-b5cc-439634f5c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ann_1 = samp_ann.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1338dc89-5540-4867-95a6-3d0ea7639cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1  = samp_ann.layers[1].output.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a92b1e3-d9c6-4ce7-959d-aef1376a29d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "427b0370-90df-4ab4-85b9-691d082750bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06982568-e5c1-4712-895c-7b3d49a09040",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a71c4646-23a6-45ca-9ccc-aa4c3566d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann_params = samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65ad268f-3ed5-4459-b5e4-4eb40f2e0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5f741d7-ba99-4803-8441-5bf2ad3d72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_X_t(data1, data2, data3, data4, size_ens, var_weights = 1.0, var_weight_weights = 4.0, var_L = 1.0, var_D = 1.0):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    \n",
    "    initial_ensembles1 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out1, data1_stack1 = get_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles2 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out2, data1_stack2 = get_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles3 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out1, data2_stack1 = get_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles4 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out2, data2_stack2 = get_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles_for_weights = generate_initial_ensembles(4, var_weight_weights, size_ens)\n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    # initial_ensembles_for_L = generate_initial_ensembles(4, var_L, size_ens)\n",
    "    # initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)    \n",
    "    \n",
    "    initial_ensembles_for_D1 = generate_initial_ensembles(1, var_D, size_ens).reshape(-1,1)\n",
    "    # initial_ensembles_for_D2 = generate_initial_ensembles(1, var_D, size_ens).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D1_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    initial_ensembles_for_D2_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D3_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.concatenate((np.expand_dims(initial_ensembles_for_D1,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D1_zero,1), \n",
    "                                                      np.expand_dims(initial_ensembles_for_D2_zero,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D3_zero,1)), axis = 2)\n",
    "    \n",
    "    # print(X_t.shape, initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4))\n",
    "    \n",
    "    return X_t, initial_ensembles, initial_ensembles_for_weights[:,0,:], initial_ensembles_for_D[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ac49c8c-6da0-4ee2-9561-e1d19365f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_targets_with_weights(batch_data, initial_ensembles, size_ens, weights): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    final_output_1 = final_output_1*weights\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5a2f9ca-f4b4-445a-ab5f-e0f0557f54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44ed09c8-f6b0-4b40-86e0-1975541fd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fudging_beta = beta(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d1a4f5e-7a0d-42b7-ad45-59c708b7b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation(data1, data2, data3, data4, combined_ensembles , size_ens, fudging_beta):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    # initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4):(4*params + 4 + 4 )]\n",
    "    \n",
    "    \n",
    "    softmax_weights = softmax(initial_ensembles_for_weights)\n",
    "    \n",
    "    # +  fudging_beta.rvs(size_ens).reshape(-1,1)\n",
    "    \n",
    "    model_1 = softmax_weights[:, 0].reshape(-1,1) \n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 1].reshape(-1,1) \n",
    "    \n",
    "    model_3 = softmax_weights[:, 2].reshape(-1,1) \n",
    "    \n",
    "    model_4 = softmax_weights[:, 3].reshape(-1,1)\n",
    "    \n",
    "    sum_weights = model_1 + model_2 + model_3 + model_4\n",
    "    \n",
    "    \n",
    "    # model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "    model_1 = model_1/sum_weights\n",
    "    \n",
    "    model_2 = model_2/sum_weights\n",
    "    \n",
    "    model_3 = model_3/sum_weights\n",
    "    \n",
    "    model_4 = model_4/sum_weights\n",
    "    \n",
    "    \n",
    "    # print(np.mean(model_1 + model_2))\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_2)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_3)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_4)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    # initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    final_output = data1_out1 + data1_out2 + data2_out1 + data2_out2\n",
    "    \n",
    "    # weighted_psa = data1_out2 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles,final_output, model_1, model_2, model_3, model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6bd2eb3-86b7-41ed-81b4-4dd8e44e106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation_test(data1, data2, data3, data4, combined_ensembles , size_ens):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    # initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    \n",
    "    softmax_weights = softmax(initial_ensembles_for_weights)\n",
    "    \n",
    "    model_1 = softmax_weights[:, :1].reshape(-1,1)\n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 1:2].reshape(-1,1) \n",
    "    \n",
    "    model_3 = softmax_weights[:, 2:3].reshape(-1,1) \n",
    "    \n",
    "    model_4 = softmax_weights[:, 3:4].reshape(-1,1)\n",
    "    \n",
    "    sum_weights = model_1 + model_2 + model_3 + model_4\n",
    "    \n",
    "    \n",
    "    # model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "    model_1 = model_1/sum_weights\n",
    "    \n",
    "    model_2 = model_2/sum_weights\n",
    "    \n",
    "    model_3 = model_3/sum_weights\n",
    "    \n",
    "    model_4 = model_4/sum_weights\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_2)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_3)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_4)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    # initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    final_output = data1_out1 + data1_out2 + data2_out1 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles, final_output, model_1, model_2, model_3, model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af3dd570-7ab3-4eae-bd8e-9a57a333b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = 4*(samp_ann.count_params() + 1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e820b010-4396-4b7a-8781-a8e6d503aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5fbaaf7-afae-4008-a26c-e0691ef9b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = total_weights//reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5111536-1a97-4390-9658-c9848a136a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f598992-73d6-4b5a-906d-3184f1b9625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_t = [[1, 1, 1, 1]]\n",
    "G_t = np.array(G_t).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17ef8c41-26ff-4e49-98f8-8f6c31b4eb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a84cb0e3-2f5e-4207-b092-d2768b52205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data1, data2, data3, data4, initial_ensembles, fudging_beta  =fudging_beta): \n",
    "    _,_, weighted_alogp, w1, w2, w3, w4 = forward_operation(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    return weighted_alogp, w1, w2, w3, w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37b60b37-92b1-4784-85fc-59a865d2398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_test(data1, data2, data3, data4, initial_ensembles): \n",
    "    _,_, weighted_alogp, w1, w2, w3, w4 = forward_operation_test(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens)\n",
    "    return weighted_alogp, w1, w2, w3, w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3af0a9b9-6202-40e2-911f-3c890213099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mu_bar_G_bar(data1, data2, data3, data4, initial_ensembles, fudging_beta):\n",
    "    H_t = np.hstack((np.identity(data1.shape[0]), np.zeros((data1.shape[0], samp_ann_params + 1  + 1))))\n",
    "    mu_bar = initial_ensembles.mean(0)\n",
    "    X_t, _,_, _, _, _, _ = forward_operation(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    X_t = X_t.transpose((0,2,1))\n",
    "    X_t = X_t.reshape(X_t.shape[0], X_t.shape[1]*X_t.shape[2])\n",
    "    script_H_t = np.kron(G_t.T, H_t)\n",
    "    G_u = (script_H_t@X_t.T)\n",
    "    G_u = G_u.T\n",
    "    G_bar = (G_u.mean(0)).ravel()\n",
    "    return mu_bar.reshape(-1,1), G_bar.reshape(-1,1), G_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7334c80c-cfcd-4d5b-b844-11cfb8c137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u): \n",
    "    u_j_minus_u_bar = initial_ensembles - mu_bar.reshape(1,-1)\n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    c = np.zeros((total_weights, G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        c += np.kron(u_j_minus_u_bar[i, :].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return c/size_ens, G_u_minus_G_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9148779c-1a3f-455d-a6ad-00fc1bf69c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_D_u( G_bar, G_u): \n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    d = np.zeros((G_bar.shape[0], G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        d += np.kron(G_u_minus_G_bar[i,:].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return d/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e8459b0-68c1-4d1f-851e-3d7b99f0ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_ensemble(data1, data2, data3, data4, initial_ensembles, y_train, size_ens = size_ens, inflation_factor = 1.0, fudging_beta = fudging_beta, \n",
    "                        fudging_var = None):\n",
    "    mu_bar, G_bar, G_u = calculate_mu_bar_G_bar(data1, data2, data3, data4, initial_ensembles, fudging_beta)\n",
    "    C, G_u_minus_G_bar = calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u)\n",
    "    D = calculate_D_u( G_bar, G_u)\n",
    "    _, R_t = create_cov(data1.shape[0],initial_ensembles)\n",
    "    inflation = np.identity(R_t.shape[0])*inflation_factor\n",
    "    D_plus_cov = D + (R_t *inflation_factor)\n",
    "    D_plus_cov_inv = np.linalg.inv(D_plus_cov)\n",
    "    mid_quant = C@D_plus_cov_inv\n",
    "    noise_vec_mean = np.zeros((R_t.shape[0], ))\n",
    "    noise_mvn = mvn(noise_vec_mean, R_t)\n",
    "    fudging = noise_mvn.rvs(size_ens)\n",
    "    interim = (y_train.T.flatten().reshape(1,-1) + fudging)\n",
    "    right_quant = interim - G_u\n",
    "    mid_times_right = mid_quant@right_quant.T\n",
    "    updated_ensemble = (initial_ensembles + mid_times_right.T)\n",
    "    if fudging_var is not None: \n",
    "        mean_vec = np.zeros((updated_ensemble.shape[1],))\n",
    "        cov_mat = np.identity(updated_ensemble.shape[1])*fudging_var\n",
    "        fudging_for_updated_ensembles = mvn(mean_vec, cov_mat)\n",
    "        fudging_for_updated_ensembles_vec = fudging_for_updated_ensembles.rvs(size_ens)\n",
    "        updated_ensemble = updated_ensemble + fudging_for_updated_ensembles_vec\n",
    "    return updated_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c14b236d-6ebf-40e3-9085-97afd2990e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "346d07bc-5b61-4e75-a816-a6e22ffe9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_D = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5abd35fb-ea92-449d-894a-162ed1f4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3016d9f-e80f-4477-b8cc-d98803aa038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return np.log(np.exp(x) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "381fc654-8877-41fd-840a-4b35153f923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cov(shape, initial_ensembles):\n",
    "    cov_part = initial_ensembles[:, -4:-3]\n",
    "    cov_part = cov_part.mean(0)\n",
    "    variances1 = softplus(cov_part)\n",
    "    n = shape\n",
    "    return variances1, np.identity(n)*variances1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c59ea29-6484-40d5-8777-80cf9ca6cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7649cb4e-4909-404a-86db-cfb8fc2e078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..//Data//gcn_cdr_train_pca.pickle\", \"rb\") as f: \n",
    "    catch_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cdfe276-982a-4469-89da-2bf969cc9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catch_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f029a85a-6790-4635-b02f-a8dfd9346e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..//Data//gcn_cdr_test_pca.pickle\", \"rb\") as f: \n",
    "    catch_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1cbefaa8-c226-47fb-a972-1a8207d29766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "176445fb-4f30-421a-a710-339955111baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_train(catch_train, size): \n",
    "    idxes = random.sample(range(0, catch_train[0].shape[0]), k = size)\n",
    "    idxes = list(idxes)\n",
    "    data1, data2, data3, data4 = catch_train[0][idxes,:], catch_train[1][idxes,:], catch_train[2][idxes,:], catch_train[3][idxes,:]\n",
    "    \n",
    "    y_train = catch_train[-1][idxes].reshape(-1,1)\n",
    "    \n",
    "    return data1, data2, data3, data4, y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7c13b4e-c9b7-4b7f-bd6e-92ec93cd40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_test( catch_test, size): \n",
    "    idxes = random.sample(range(0, catch_test[0].shape[0]), k = size)\n",
    "    idxes = list(idxes)\n",
    "    data1, data2, data3, data4 = catch_test[0][idxes,:], catch_test[1][idxes,:], catch_test[2][idxes,:], catch_test[3][idxes,:]\n",
    "    y_train = catch_test[-1][idxes].reshape(-1,1)\n",
    "    return data1, data2, data3, data4, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2a4da04-1a68-4b91-ad62-e6c027c91fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1_train, data2_train, data3_train, data4_train, y_train =  prepare_data_train(catch_train, size = 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e54f80e-5b8d-4f46-a24a-2514a3683a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1_test, data2_test, data3_test, data4_test, y_test =  prepare_data_test(catch_test, size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aadbd46f-4890-40cc-8176-f292b9c45cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9550dacb-e351-4e82-aace-ec44d6b9bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0b26fdd2-0cf7-4d6f-9e1b-33f89e4ba9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a2649197-8644-44db-beac-ed6704cd37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0923af84-5798-41eb-b458-3de6028f067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     data1_train, data2_train, data3_train, data4_train, y_train =  prepare_data_train(catch_train, size = 10000)\n",
    "\n",
    "#     train_idxes = random.sample(range(0, data1_train.shape[0]), k = data1_train.shape[0])\n",
    "    \n",
    "#     train_chunks = list(chunks(train_idxes, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4136098-8379-4885-80fc-0a84d43fa5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ce92a4a-1b21-4c34-b7a2-ce06d78e2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65bf21e9-92c6-4824-93c3-87274b072afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(idx, var_weights = 1.0, var_weight_weights = 4.0, var_D = 1, inflation_factor = 1.6, fudging_beta = beta(1,19), \n",
    "               fudging_var = 1e-3, epochs = 30):\n",
    "    \n",
    "    # smiles_feats_train, rdkit_feats_train, smiles_feats_valid, rdkit_feats_valid, y_train, y_train_actual, y_valid, y_valid_actual, initial_ensembles  = prepare_data(idx, var_weights = var_weights, var_weight_weights =var_weight_weights, var_L = var_L, var_D = var_D)\n",
    "    \n",
    "    \n",
    "    data1_train, data2_train, data3_train, data4_train, y_train =  prepare_data_train(catch_train, size = 5000)\n",
    "\n",
    "    train_idxes = random.sample(range(0, data1_train.shape[0]), k = data1_train.shape[0])\n",
    "    \n",
    "    train_chunks = list(chunks(train_idxes, batch_size))\n",
    "    \n",
    "    data1_test, data2_test, data3_test, data4_test, y_test =  prepare_data_test(catch_test, size = 2500)\n",
    "    \n",
    "    X_t, initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_D = get_initial_X_t(data1_train, data2_train, data3_train, data4_train,\n",
    "                                                                                                 size_ens = size_ens, var_weights = var_weights,\n",
    "                                                                                                var_weight_weights = var_weight_weights,\n",
    "                                                                                                     var_D = var_D)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    best_rmse_train = 100000\n",
    "    \n",
    "    patience = 0\n",
    "    \n",
    "    for i in range(0,epochs):\n",
    "        # print(\"epoch number is \" +str(i))\n",
    "        \n",
    "        for chunk in train_chunks:\n",
    "            \n",
    "            data1_train1, data2_train1, data3_train1, data4_train1, y_train1 = data1_train[chunk,:], data2_train[chunk,:], data3_train[chunk,:], data4_train[chunk,:], y_train[chunk,:]\n",
    "\n",
    "            initial_ensembles = get_updated_ensemble(data1_train1, data2_train1, data3_train1, data4_train1, initial_ensembles, y_train1, size_ens = size_ens,\n",
    "                                                 inflation_factor = inflation_factor, fudging_beta = fudging_beta, fudging_var = fudging_var)\n",
    "        \n",
    "            G_u_train, w1, w2, w3, w4 = get_predictions(data1_train1, data2_train1, data3_train1, data4_train1, initial_ensembles, fudging_beta)\n",
    "    \n",
    "            li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)    \n",
    "            ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)  \n",
    "    \n",
    "            width_train = ui_train - li_train\n",
    "            avg_width_train = width_train.mean(0)[0]\n",
    "    \n",
    "            ind_train = (y_train1 >= li_train) & (y_train1 <= ui_train)\n",
    "            coverage_train= ind_train.mean(0)[0]\n",
    "    \n",
    "            averaged_targets_train = G_u_train.mean(0).reshape(-1,1)\n",
    "            rmse_train = np.sqrt(((y_train1 -averaged_targets_train)**2).mean(0))[0]\n",
    "        \n",
    "            pearsonr_train = pearsonr(averaged_targets_train.reshape(averaged_targets_train.shape[0],), \n",
    "                                 y_train1.reshape(y_train1.shape[0],))\n",
    "        \n",
    "            r_train = pearsonr_train.statistic\n",
    "    \n",
    "            G_u_test, _, _, _, _ = get_predictions_test(data1_test, data2_test, data3_test, data4_test, initial_ensembles)\n",
    "    \n",
    "\n",
    "    \n",
    "            li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)     \n",
    "            ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)   \n",
    "    \n",
    "            width_test = ui_test - li_test\n",
    "            avg_width_test = width_test.mean(0)[0]\n",
    "    \n",
    "            ind_test = (y_test >= li_test) & (y_test <= ui_test)\n",
    "            coverage_test= ind_test.mean(0)[0]\n",
    "    \n",
    "            averaged_targets_test = G_u_test.mean(0).reshape(-1,1)\n",
    "            rmse_test = np.sqrt(((y_test -averaged_targets_test)**2).mean(0))[0]  \n",
    "        \n",
    "            pearsonr_test = pearsonr(averaged_targets_test.reshape(averaged_targets_test.shape[0],), \n",
    "                                 y_test.reshape(y_test.shape[0],))\n",
    "        \n",
    "            r_test = pearsonr_test.statistic\n",
    "\n",
    "            # print(\"Training Coverage, Widths, RMSE, and Pearson R\")\n",
    "            # print(coverage_train, avg_width_train, rmse_train, r_train)\n",
    "            # print(\"Testing Coverage, Widths, RMSE, and Pearson R\")\n",
    "            # print(coverage_test, avg_width_test, rmse_test, r_test)\n",
    "            # print(\"Sum of Average Model Weights\")\n",
    "            # print(w1.mean() + w2.mean() + w3.mean() + w4.mean())\n",
    "            # print(\"Sum of Average Model 1 Weights\")\n",
    "            # print(w1.mean(), w1.std())\n",
    "            # print(\"Sum of Average Model 2 Weights\")\n",
    "            # print(w2.mean(), w2.std())\n",
    "            # print(\"Sum of Average Model 3 Weights\")\n",
    "            # print(w3.mean(), w3.std())\n",
    "            # print(\"Sum of Average Model 4 Weights\")\n",
    "            # print(w4.mean(), w4.std())\n",
    "\n",
    "            if (rmse_train < best_rmse_train):\n",
    "                best_pearsonr = r_train\n",
    "                best_rmse_train = rmse_train\n",
    "            # print(\"went here\")\n",
    "                best_train_width_mean = avg_width_train.mean()\n",
    "                best_train_width = avg_width_train\n",
    "            # best_smiles_weight = w1.mean()\n",
    "                best_coverage_train = coverage_train\n",
    "                best_rmse_train = rmse_train\n",
    "                best_pearson_r = r_test\n",
    "                best_test_width = avg_width_test\n",
    "\n",
    "                best_coverage_test = coverage_test    \n",
    "                best_rmse_test = rmse_test\n",
    "                patience = 0\n",
    "                best_ensembles = initial_ensembles\n",
    "            \n",
    "            else:\n",
    "                patience = patience + 1\n",
    "            \n",
    "            # print(\"Patience is\")\n",
    "            # print(patience)\n",
    "            # print('\\n')\n",
    "            \n",
    "            if (patience > threshold) | (i == (epochs-1)):\n",
    "            \n",
    "            # print()\n",
    "            # print(best_train_width.tolist(), best_coverage_train.tolist(), best_rmse_train.tolist(), best_test_width.tolist(), best_coverage_test.tolist(), best_rmse_test.tolist(), best_smiles_weight, flush = True)\n",
    "            # print(\"done for fold\" + str(idx), flush = True)\n",
    "                print(\"train_coverage\" + str(best_coverage_train), flush = True)\n",
    "                print(\"test_coverage\" + str(best_coverage_test), flush = True)\n",
    "                print(\"train_width\" + str(best_train_width), flush = True)\n",
    "                print(\"test_width\" + str(best_test_width), flush = True)\n",
    "                print(\"pearson\" + str(best_pearson_r), flush = True)\n",
    "                print(\"rmse_train\" + str(best_rmse_train), flush = True)\n",
    "                print(\"rmse_test\" + str(best_rmse_test), flush = True)\n",
    "            # print(\"smiles_weight_ci\" + str([best_li_smiles_weight, best_ui_smiles_weight]), flush = True)\n",
    "            \n",
    "                return [best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r, best_ensembles, [data1_test, data2_test, data3_test, data4_test, y_test]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "11a8c623-2952-409f-a640-7d89cef6b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "909888fc-c2b2-49e9-b0f0-887c8fa4d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r, best_ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11aec33b-bbd2-4a4c-8467-b47e822b3525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "348d2126-693a-4e12-9c34-3be30dc4b535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_coverage0.95\n",
      "test_coverage0.902\n",
      "train_width5.568277850363637\n",
      "test_width5.461640131480348\n",
      "pearson0.8400672347785835\n",
      "rmse_train1.3252794080747985\n",
      "rmse_test1.5376562433321106\n",
      "CPU times: user 6min 24s, sys: 4min 9s, total: 10min 34s\n",
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.568277850363637,\n",
       " 0.95,\n",
       " 1.3252794080747985,\n",
       " 5.461640131480348,\n",
       " 0.902,\n",
       " 1.5376562433321106,\n",
       " 0.8400672347785835,\n",
       " array([[ 0.48334338, -0.99512841,  0.39591043, ..., -0.30359027,\n",
       "         -0.8884931 , -0.07814189],\n",
       "        [ 0.30378543, -1.18250323,  0.97320598, ..., -0.5084828 ,\n",
       "         -1.38871392, -0.25467774],\n",
       "        [ 0.00481866, -1.0408983 ,  0.97533784, ..., -0.56634768,\n",
       "         -1.1075571 , -0.14250837],\n",
       "        ...,\n",
       "        [ 0.42899007, -0.87590464,  0.71549593, ..., -0.29716172,\n",
       "         -1.08445111,  0.20729504],\n",
       "        [ 0.58795435, -0.61705008,  0.73656062, ..., -0.18382558,\n",
       "         -0.90743687, -0.11265691],\n",
       "        [ 0.57179665, -0.68411289,  1.03875533, ..., -0.08806836,\n",
       "         -0.89057015, -0.17749938]]),\n",
       " [array([[-31.0558849 ,  -7.69812794,   7.2790794 , ...,   6.60993213,\n",
       "            1.34485188,   1.49409485],\n",
       "         [  8.32017559,  17.57044508, -15.79252803, ...,  24.45345593,\n",
       "            2.1559565 , -10.98377131],\n",
       "         [ -8.02815969,  -2.32046924,  -4.37724259, ...,   2.78840481,\n",
       "           -6.36874388,   2.13972278],\n",
       "         ...,\n",
       "         [-25.50150524,   6.43092548,  -6.60048557, ...,   0.36355487,\n",
       "           -5.96933694,  -1.01828606],\n",
       "         [-39.59961588, -12.68545662,  -5.52474498, ...,   0.50934684,\n",
       "            4.28685227,   5.8398103 ],\n",
       "         [ 10.90960305,  75.94829218,  -3.70958886, ...,   1.20229486,\n",
       "           -1.50385822,   0.35127033]]),\n",
       "  array([[ 0.87074985, -7.50763367, -2.80573934, ...,  1.43661608,\n",
       "          -1.51415088, -0.48440117],\n",
       "         [ 1.12555388,  0.03761683,  3.03992252, ...,  0.17542496,\n",
       "           0.02397398, -0.68932152],\n",
       "         [ 1.23548727, -3.28834102, -1.1895497 , ...,  0.29698906,\n",
       "          -0.82571168, -0.26844555],\n",
       "         ...,\n",
       "         [ 0.47767366, -3.00805574, -2.46391121, ...,  0.15827314,\n",
       "          -0.16305604, -0.16247248],\n",
       "         [ 2.2010101 , -3.61036001, -0.81111025, ..., -0.39581955,\n",
       "          -0.34039283, -0.18091168],\n",
       "         [ 2.43414015, -3.32882008, -1.33069305, ...,  0.97258911,\n",
       "          -0.52168946,  0.13230416]]),\n",
       "  array([[-1.15095247, -0.39151048, -0.58112753, ...,  0.19708087,\n",
       "          -0.03770775,  0.28468274],\n",
       "         [ 0.43721288,  1.00456402,  0.74387429, ..., -0.14228022,\n",
       "          -0.53557815, -0.37430801],\n",
       "         [-0.21218125, -0.14405061, -0.50782611, ..., -0.00245778,\n",
       "          -0.04018319,  0.33106682],\n",
       "         ...,\n",
       "         [-1.19357603,  0.03384474, -0.53392816, ...,  0.00998123,\n",
       "          -0.00970717, -0.07329554],\n",
       "         [-1.54483466, -1.25833192, -1.16397391, ...,  0.12418945,\n",
       "           0.18603569, -0.08323434],\n",
       "         [-0.97048748,  0.94940107,  1.19571007, ...,  0.06815669,\n",
       "          -0.18955723, -0.02720948]]),\n",
       "  array([[ 6.36003098e+00,  1.78819093e+00, -3.93733116e-01, ...,\n",
       "          -3.93331832e-02,  8.50565663e-02, -1.87722112e-01],\n",
       "         [-4.12485517e+00,  1.56121234e+00, -5.80644574e-01, ...,\n",
       "           1.44524722e-01, -1.14373595e-01,  1.07329346e-01],\n",
       "         [-4.46579109e-01, -4.40773577e+00,  4.67802665e-01, ...,\n",
       "          -4.70481461e-02, -1.02482961e-01,  3.87383632e-02],\n",
       "         ...,\n",
       "         [-2.39710017e+00,  5.46623034e+00,  1.91933188e+00, ...,\n",
       "          -1.41213373e-01, -4.20583086e-02,  5.74767414e-02],\n",
       "         [ 6.88590831e+00,  2.90897607e+00, -7.83508582e-01, ...,\n",
       "           3.46064843e-02, -2.47094217e-01,  7.46387621e-04],\n",
       "         [ 3.13028205e+00, -3.32516105e+00, -6.98103982e-02, ...,\n",
       "          -4.27860429e-02,  1.44390515e-01,  2.95002836e-01]]),\n",
       "  array([[3.572556],\n",
       "         [3.618906],\n",
       "         [2.263595],\n",
       "         ...,\n",
       "         [3.789891],\n",
       "         [1.732312],\n",
       "         [4.918302]])]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_results(0, var_weights = 3.0, var_weight_weights = 3.0, var_D = 1, inflation_factor =1, fudging_beta = beta(1,19), \n",
    "           fudging_var = 1e-2, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d3ec775c-3087-4ecf-a48e-9677d9ae6888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Using backend LokyBackend with 14 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_coverage0.966\n",
      "test_coverage0.9344\n",
      "train_width6.608032484420482\n",
      "test_width6.563397024665629\n",
      "pearson0.8299583558779688\n",
      "rmse_train1.3663738738380578\n",
      "rmse_test1.6113758722191907\n",
      "train_coverage1.0\n",
      "test_coverage1.0\n",
      "train_width30.60971448059724\n",
      "test_width30.859966188268114\n",
      "pearson0.7737809551773488\n",
      "rmse_train1.8526858997565114\n",
      "rmse_test1.866877290531824\n",
      "train_coverage0.9\n",
      "test_coverage0.842\n",
      "train_width4.541081824243266\n",
      "test_width4.604482851394375\n",
      "pearson0.8373412088569717\n",
      "rmse_train1.3522170477168507\n",
      "rmse_test1.5966398481956383\n",
      "train_coverage0.876\n",
      "test_coverage0.8044\n",
      "train_width4.2249423387065645\n",
      "test_width4.133192462495216\n",
      "pearson0.8219545506942935\n",
      "rmse_train1.3160074606461805\n",
      "rmse_test1.6091740159809609\n",
      "train_coverage0.978\n",
      "test_coverage0.9664\n",
      "train_width7.288440986078596\n",
      "test_width7.267751692319305\n",
      "pearson0.8430914734534328\n",
      "rmse_train1.3439381536490935\n",
      "rmse_test1.554877152986873\n",
      "train_coverage0.946\n",
      "test_coverage0.938\n",
      "train_width6.168589657041274\n",
      "test_width6.138293370496912\n",
      "pearson0.8486323202256891\n",
      "rmse_train1.3288006555963336\n",
      "rmse_test1.4853244414832658\n",
      "train_coverage0.846\n",
      "test_coverage0.8056\n",
      "train_width4.327282192738341\n",
      "test_width4.309749591361025\n",
      "pearson0.8360877014621774\n",
      "rmse_train1.3335057945412339\n",
      "rmse_test1.5558388727059558\n",
      "train_coverage0.956\n",
      "test_coverage0.934\n",
      "train_width6.407618642161583\n",
      "test_width6.492818955715481\n",
      "pearson0.8198371458015664\n",
      "rmse_train1.3749832745426758\n",
      "rmse_test1.5909537211707316\n",
      "train_coverage0.81\n",
      "test_coverage0.802\n",
      "train_width4.201529169903161\n",
      "test_width4.262168621584447\n",
      "pearson0.8368317763038119\n",
      "rmse_train1.3420546088879661\n",
      "rmse_test1.5665118963770464\n",
      "train_coverage0.976\n",
      "test_coverage0.9696\n",
      "train_width7.83360093686149\n",
      "test_width7.947802629966105\n",
      "pearson0.8376089205672382\n",
      "rmse_train1.3476380571323265\n",
      "rmse_test1.5471296480412278\n",
      "train_coverage0.904\n",
      "test_coverage0.8464\n",
      "train_width4.4586048384194665\n",
      "test_width4.536595923399512\n",
      "pearson0.8410647323862275\n",
      "rmse_train1.2188390358913999\n",
      "rmse_test1.5347404807681324\n",
      "train_coverage0.962\n",
      "test_coverage0.9296\n",
      "train_width5.9943079037119675\n",
      "test_width6.115520648206986\n",
      "pearson0.82883581451187\n",
      "rmse_train1.3165356019008354\n",
      "rmse_test1.583988656045467\n",
      "train_coverage0.978\n",
      "test_coverage0.9608\n",
      "train_width7.343978102663277\n",
      "test_width7.643298104049318\n",
      "pearson0.8264321725519739\n",
      "rmse_train1.3334008224183578\n",
      "rmse_test1.5945544394465447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Done  13 tasks      | elapsed:  7.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_coverage0.848\n",
      "test_coverage0.8008\n",
      "train_width4.0583735620286765\n",
      "test_width4.144638582898742\n",
      "pearson0.838667978297117\n",
      "rmse_train1.3241922213482786\n",
      "rmse_test1.5390028461231322\n",
      "train_coverage0.994\n",
      "test_coverage0.9776\n",
      "train_width8.092412671749514\n",
      "test_width8.117871158600787\n",
      "pearson0.827290085079035\n",
      "rmse_train1.4237815379654715\n",
      "rmse_test1.6069752107674815\n",
      "train_coverage0.986\n",
      "test_coverage0.9548\n",
      "train_width7.187179640168668\n",
      "test_width7.290734016821086\n",
      "pearson0.8295064151513025\n",
      "rmse_train1.3223665572372574\n",
      "rmse_test1.5749176689576627\n",
      "train_coverage0.876\n",
      "test_coverage0.8084\n",
      "train_width4.281927519377705\n",
      "test_width4.299112124021357\n",
      "pearson0.826695530560799\n",
      "rmse_train1.3045281740237755\n",
      "rmse_test1.5964986086310893\n",
      "train_coverage0.836\n",
      "test_coverage0.7988\n",
      "train_width3.966985037630957\n",
      "test_width4.142578482661991\n",
      "pearson0.8504656683812725\n",
      "rmse_train1.3542250514553316\n",
      "rmse_test1.5091225641954458\n",
      "train_coverage1.0\n",
      "test_coverage0.9976\n",
      "train_width11.89748162338938\n",
      "test_width11.93659328863632\n",
      "pearson0.8123283482495569\n",
      "rmse_train1.433861424319916\n",
      "rmse_test1.6302483198620208\n",
      "train_coverage0.998\n",
      "test_coverage0.9984\n",
      "train_width12.54501827896021\n",
      "test_width12.966484722484221\n",
      "pearson0.8383977832597556\n",
      "rmse_train1.423892063039401\n",
      "rmse_test1.5461564446419782\n",
      "train_coverage0.998\n",
      "test_coverage0.9984\n",
      "train_width15.284307299244563\n",
      "test_width15.455884620061063\n",
      "pearson0.8118649136177943\n",
      "rmse_train1.4564282741824375\n",
      "rmse_test1.6713442160870473\n",
      "train_coverage0.838\n",
      "test_coverage0.7896\n",
      "train_width4.018185992943715\n",
      "test_width4.1138092551880625\n",
      "pearson0.844993702684391\n",
      "rmse_train1.3446750114923558\n",
      "rmse_test1.570666814778844\n",
      "train_coverage0.828\n",
      "test_coverage0.802\n",
      "train_width4.1730037457758336\n",
      "test_width4.10856912633709\n",
      "pearson0.8400517374458413\n",
      "rmse_train1.383465304777865\n",
      "rmse_test1.5252538033533045\n",
      "train_coverage0.908\n",
      "test_coverage0.8596\n",
      "train_width4.731994955845921\n",
      "test_width4.75074635730404\n",
      "pearson0.8301622982697248\n",
      "rmse_train1.3011734835804278\n",
      "rmse_test1.585803856150259\n",
      "train_coverage0.94\n",
      "test_coverage0.9148\n",
      "train_width5.786441084594367\n",
      "test_width5.8770259771047275\n",
      "pearson0.8364450295282894\n",
      "rmse_train1.3360223915709613\n",
      "rmse_test1.5566816183590135\n",
      "train_coverage0.904\n",
      "test_coverage0.8364\n",
      "train_width4.738274950229879\n",
      "test_width4.61509446907023\n",
      "pearson0.8462130860075027\n",
      "rmse_train1.3000408514863315\n",
      "rmse_test1.5223649847670948\n",
      "train_coverage0.864\n",
      "test_coverage0.81\n",
      "train_width4.184451713892692\n",
      "test_width4.132366379169078\n",
      "pearson0.8430144547418057\n",
      "rmse_train1.3390789530867657\n",
      "rmse_test1.494638520306461\n",
      "train_coverage0.982\n",
      "test_coverage0.9604\n",
      "train_width7.765934541393384\n",
      "test_width7.58881576897852\n",
      "pearson0.8216499051945114\n",
      "rmse_train1.3205743359227635\n",
      "rmse_test1.616194350554243\n",
      "train_coverage0.938\n",
      "test_coverage0.8964\n",
      "train_width5.300666674110118\n",
      "test_width5.312028936681018\n",
      "pearson0.8387673036212457\n",
      "rmse_train1.3306585674515075\n",
      "rmse_test1.5755322364887505\n",
      "train_coverage0.996\n",
      "test_coverage0.9944\n",
      "train_width11.310442918916925\n",
      "test_width11.309841050092121\n",
      "pearson0.8179193988952502\n",
      "rmse_train1.3970723635211049\n",
      "rmse_test1.6028413080853074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Done  30 out of  50 | elapsed: 18.4min remaining: 12.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_coverage0.998\n",
      "test_coverage0.9976\n",
      "train_width15.137881966821071\n",
      "test_width15.542642280222498\n",
      "pearson0.8043110775941537\n",
      "rmse_train1.499011485944278\n",
      "rmse_test1.6489812520468152\n",
      "train_coverage0.924\n",
      "test_coverage0.886\n",
      "train_width5.405764874297754\n",
      "test_width5.4504590830509345\n",
      "pearson0.8202299380971108\n",
      "rmse_train1.3414846113537027\n",
      "rmse_test1.6141278793545961\n",
      "train_coverage0.96\n",
      "test_coverage0.944\n",
      "train_width6.7045071707987285\n",
      "test_width6.718709034964367\n",
      "pearson0.8377786388031145\n",
      "rmse_train1.3549520986555472\n",
      "rmse_test1.5270997506758712\n",
      "train_coverage0.94\n",
      "test_coverage0.926\n",
      "train_width6.386298288203113\n",
      "test_width6.450920587128132\n",
      "pearson0.8253000067863437\n",
      "rmse_train1.3584918784656665\n",
      "rmse_test1.5719814347627312\n",
      "train_coverage0.99\n",
      "test_coverage0.9848\n",
      "train_width8.213508168211142\n",
      "test_width8.294330483474921\n",
      "pearson0.8456260726233374\n",
      "rmse_train1.3737472952959675\n",
      "rmse_test1.5002058032382168\n",
      "train_coverage0.964\n",
      "test_coverage0.9384\n",
      "train_width7.162217054411263\n",
      "test_width7.013046909579591\n",
      "pearson0.8298080641135918\n",
      "rmse_train1.3119399615848146\n",
      "rmse_test1.5982463848592\n",
      "train_coverage0.972\n",
      "test_coverage0.9432\n",
      "train_width6.475269629417868\n",
      "test_width6.5509158960236755\n",
      "pearson0.8464781179482106\n",
      "rmse_train1.3374332388676566\n",
      "rmse_test1.5026515203568245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Done  37 out of  50 | elapsed: 22.3min remaining:  7.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_coverage0.872\n",
      "test_coverage0.8484\n",
      "train_width4.617718249246711\n",
      "test_width4.682201844759479\n",
      "pearson0.8315816521450089\n",
      "rmse_train1.3900364125900757\n",
      "rmse_test1.5658173238918645\n",
      "train_coverage0.962\n",
      "test_coverage0.9416\n",
      "train_width6.744350290844302\n",
      "test_width6.624109314853002\n",
      "pearson0.8238252525279334\n",
      "rmse_train1.3180738179137441\n",
      "rmse_test1.5696138641216562\n",
      "train_coverage0.868\n",
      "test_coverage0.8448\n",
      "train_width4.645975721765668\n",
      "test_width4.713544147122734\n",
      "pearson0.8352164758646092\n",
      "rmse_train1.3729714502071433\n",
      "rmse_test1.5427485639280794\n",
      "train_coverage0.96\n",
      "test_coverage0.9388\n",
      "train_width6.5861860353762935\n",
      "test_width6.514607431416811\n",
      "pearson0.8151236571204241\n",
      "rmse_train1.3616962967297859\n",
      "rmse_test1.600566949071685\n",
      "train_coverage0.856\n",
      "test_coverage0.8204\n",
      "train_width4.510040392374692\n",
      "test_width4.258101356195286\n",
      "pearson0.8417563882448914\n",
      "rmse_train1.3354474021241127\n",
      "rmse_test1.5479450812774307\n",
      "train_coverage0.958\n",
      "test_coverage0.906\n",
      "train_width6.047865491721992\n",
      "test_width6.045875304200652\n",
      "pearson0.8291128226150851\n",
      "rmse_train1.261903545329972\n",
      "rmse_test1.5891301789707708\n",
      "train_coverage0.884\n",
      "test_coverage0.842\n",
      "train_width4.428954004436901\n",
      "test_width4.555065653408046\n",
      "pearson0.8422725525298033\n",
      "rmse_train1.2996182390086661\n",
      "rmse_test1.5685668628472684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Done  44 out of  50 | elapsed: 24.7min remaining:  3.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_coverage0.92\n",
      "test_coverage0.9104\n",
      "train_width5.72661952396621\n",
      "test_width5.7745551200292375\n",
      "pearson0.8309018536957511\n",
      "rmse_train1.4179733948546407\n",
      "rmse_test1.5888390455158714\n",
      "train_coverage0.806\n",
      "test_coverage0.7772\n",
      "train_width3.8280580784771447\n",
      "test_width3.766947211325494\n",
      "pearson0.8443492810751236\n",
      "rmse_train1.3572820403975911\n",
      "rmse_test1.503545417839696\n",
      "train_coverage0.914\n",
      "test_coverage0.866\n",
      "train_width5.320851785118873\n",
      "test_width5.259275697297069\n",
      "pearson0.8345065960432101\n",
      "rmse_train1.298300045476426\n",
      "rmse_test1.5955625251374017\n",
      "train_coverage0.884\n",
      "test_coverage0.814\n",
      "train_width4.355276332866703\n",
      "test_width4.421617391468295\n",
      "pearson0.8358444910381956\n",
      "rmse_train1.290470929413374\n",
      "rmse_test1.5450921556949264\n",
      "train_coverage0.802\n",
      "test_coverage0.7732\n",
      "train_width3.8411690231890305\n",
      "test_width3.8302938983335464\n",
      "pearson0.8501796300237906\n",
      "rmse_train1.3842297491411149\n",
      "rmse_test1.5201527445395586\n",
      "train_coverage0.844\n",
      "test_coverage0.7908\n",
      "train_width4.255603118686237\n",
      "test_width4.137228740490755\n",
      "pearson0.8371957766978746\n",
      "rmse_train1.3591612981940207\n",
      "rmse_test1.616571228612664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=14)]: Done  50 out of  50 | elapsed: 27.6min finished\n"
     ]
    }
   ],
   "source": [
    "catch_overall = Parallel(n_jobs = 14, backend = \"loky\", verbose = 8)(delayed(get_results)(idx,  var_weights = 3.0, var_weight_weights = 3.0, var_D = 1, inflation_factor =1, fudging_beta = beta(1,19), \n",
    "           fudging_var = 1e-2, epochs = 50) for idx in range(0,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9538d375-2e01-411d-a7fb-a9781b542b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..//Data//catch_overall_50_reps_config_parallel_trial1_more_data.pickle\", \"wb\") as f: \n",
    "    pickle.dump(catch_overall,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3b58627-fbcd-426c-9593-95656b441359",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df = pd.DataFrame(catch_overall).iloc[:,:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f13fc0e2-e504-47ab-8d00-aa3ccfaecbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6.794498\n",
       "1    0.922200\n",
       "2    1.358945\n",
       "3    6.832834\n",
       "4    0.892504\n",
       "5    1.574794\n",
       "6    0.831946\n",
       "dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "695b36f6-2037-4b6a-95f1-a102d8e0ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df.columns = ['best_train_width', 'best_coverage_train', 'best_rmse_train',\n",
    "                      'best_test_width', 'best_coverage_test', 'best_rmse_test', 'best_pearson_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d83bc273-8bd3-4c6a-8273-215b7291510e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "best_train_width       6.794498\n",
       "best_coverage_train    0.922200\n",
       "best_rmse_train        1.358945\n",
       "best_test_width        6.832834\n",
       "best_coverage_test     0.892504\n",
       "best_rmse_test         1.574794\n",
       "best_pearson_r         0.831946\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_df.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
