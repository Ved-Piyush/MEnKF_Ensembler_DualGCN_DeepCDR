{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa53e19a-64db-4b1d-9378-c19c8efacd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 01:57:11.627665: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-01 01:57:11.649327: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-01 01:57:11.766241: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-01 01:57:11.767351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-01 01:57:12.672705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import block_diag\n",
    "import warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6028973-9799-4d8a-b919-a8dd6283faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34242ac3-a983-4bab-b092-bfcea4f94a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(weights_ann_1[0].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d459b853-f520-4181-a9a9-310a6dd20de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, initial_ensembles, size_ens): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    # weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    # h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a66ee4c-4112-4dca-a9cc-68a60d4e316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 32, input_shape = 256, output_shape = 1): \n",
    "    input_layer = tf.keras.layers.Input(shape = (input_shape))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(output_shape, activation = \"relu\")\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b9ef03-af61-4ce4-99ff-2f0b5dca639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_ensembles(num_weights, lambda1, size_ens):\n",
    "    mean_vec = np.zeros((num_weights,))\n",
    "    cov_matrix = lambda1*np.identity(num_weights)\n",
    "    mvn_samp = mvn(mean_vec, cov_matrix)\n",
    "    return mvn_samp.rvs(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2ea2b5-13a4-41c4-8257-c6c18708501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "758ac30a-bcc7-4b5e-9314-7fb85f284f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann =  ann(hidden = 16, input_shape = 32, output_shape = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2548f2de-a4c3-4850-b5cc-439634f5c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ann_1 = samp_ann.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1338dc89-5540-4867-95a6-3d0ea7639cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1  = samp_ann.layers[1].output.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a92b1e3-d9c6-4ce7-959d-aef1376a29d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "427b0370-90df-4ab4-85b9-691d082750bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "545"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06982568-e5c1-4712-895c-7b3d49a09040",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a71c4646-23a6-45ca-9ccc-aa4c3566d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann_params = samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5f741d7-ba99-4803-8441-5bf2ad3d72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_X_t(data1, data2, data3, data4, size_ens, var_weights = 1.0, var_weight_weights = 4.0, var_L = 1.0, var_D = 1.0):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    \n",
    "    initial_ensembles1 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out1, data1_stack1 = get_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles2 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out2, data1_stack2 = get_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles3 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out1, data2_stack1 = get_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles4 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out2, data2_stack2 = get_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles_for_weights = generate_initial_ensembles(4, var_weight_weights, size_ens)\n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    # initial_ensembles_for_L = generate_initial_ensembles(4, var_L, size_ens)\n",
    "    # initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)    \n",
    "    \n",
    "    initial_ensembles_for_D1 = generate_initial_ensembles(1, var_D, size_ens).reshape(-1,1)\n",
    "    # initial_ensembles_for_D2 = generate_initial_ensembles(1, var_D, size_ens).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D1_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    initial_ensembles_for_D2_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D3_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.concatenate((np.expand_dims(initial_ensembles_for_D1,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D1_zero,1), \n",
    "                                                      np.expand_dims(initial_ensembles_for_D2_zero,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D3_zero,1)), axis = 2)\n",
    "    \n",
    "    # print(X_t.shape, initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4))\n",
    "    \n",
    "    return X_t, initial_ensembles, initial_ensembles_for_weights[:,0,:], initial_ensembles_for_D[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ac49c8c-6da0-4ee2-9561-e1d19365f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_targets_with_weights(batch_data, initial_ensembles, size_ens, weights): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    final_output_1 = final_output_1*weights\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5a2f9ca-f4b4-445a-ab5f-e0f0557f54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44ed09c8-f6b0-4b40-86e0-1975541fd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fudging_beta = beta(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d1a4f5e-7a0d-42b7-ad45-59c708b7b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation(data1, data2, data3, data4, combined_ensembles , size_ens, fudging_beta):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    # initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4):(4*params + 4 + 4 )]\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()\n",
    "    \n",
    "    # +  fudging_beta.rvs(size_ens).reshape(-1,1)\n",
    "    \n",
    "    model_1 = softmax_weights[:, 0].reshape(-1,1) \n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 1].reshape(-1,1) \n",
    "    \n",
    "    model_3 = softmax_weights[:, 2].reshape(-1,1) \n",
    "    \n",
    "    model_4 = softmax_weights[:, 3].reshape(-1,1)\n",
    "    \n",
    "    sum_weights = model_1 + model_2 + model_3 + model_4\n",
    "    \n",
    "    \n",
    "    # model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "    model_1 = model_1/sum_weights\n",
    "    \n",
    "    model_2 = model_2/sum_weights\n",
    "    \n",
    "    model_3 = model_3/sum_weights\n",
    "    \n",
    "    model_4 = model_4/sum_weights\n",
    "    \n",
    "    \n",
    "    # print(np.mean(model_1 + model_2))\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_2)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_3)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_4)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    # initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    final_output = data1_out1 + data1_out2 + data2_out1 + data2_out2\n",
    "    \n",
    "    # weighted_psa = data1_out2 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles,final_output, model_1, model_2, model_3, model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6bd2eb3-86b7-41ed-81b4-4dd8e44e106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation_test(data1, data2, data3, data4, combined_ensembles , size_ens):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    # initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()\n",
    "    \n",
    "    model_1 = softmax_weights[:, :1].reshape(-1,1)\n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 1:2].reshape(-1,1) \n",
    "    \n",
    "    model_3 = softmax_weights[:, 2:3].reshape(-1,1) \n",
    "    \n",
    "    model_4 = softmax_weights[:, 3:4].reshape(-1,1)\n",
    "    \n",
    "    sum_weights = model_1 + model_2 + model_3 + model_4\n",
    "    \n",
    "    \n",
    "    # model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "    model_1 = model_1/sum_weights\n",
    "    \n",
    "    model_2 = model_2/sum_weights\n",
    "    \n",
    "    model_3 = model_3/sum_weights\n",
    "    \n",
    "    model_4 = model_4/sum_weights\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_2)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_3)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_4)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    # initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    final_output = data1_out1 + data1_out2 + data2_out1 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles, final_output, model_1, model_2, model_3, model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af3dd570-7ab3-4eae-bd8e-9a57a333b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = 4*(samp_ann.count_params() + 1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e820b010-4396-4b7a-8781-a8e6d503aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5fbaaf7-afae-4008-a26c-e0691ef9b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = total_weights//reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5111536-1a97-4390-9658-c9848a136a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f598992-73d6-4b5a-906d-3184f1b9625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_t = [[1, 1, 1, 1]]\n",
    "G_t = np.array(G_t).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17ef8c41-26ff-4e49-98f8-8f6c31b4eb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a84cb0e3-2f5e-4207-b092-d2768b52205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data1, data2, data3, data4, initial_ensembles, fudging_beta  =fudging_beta): \n",
    "    _,_, weighted_alogp, w1, w2, w3, w4 = forward_operation(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    return weighted_alogp, w1, w2, w3, w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37b60b37-92b1-4784-85fc-59a865d2398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_test(data1, data2, data3, data4, initial_ensembles): \n",
    "    _,_, weighted_alogp, w1, w2, w3, w4 = forward_operation_test(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens)\n",
    "    return weighted_alogp, w1, w2, w3, w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3af0a9b9-6202-40e2-911f-3c890213099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mu_bar_G_bar(data1, data2, data3, data4, initial_ensembles, fudging_beta):\n",
    "    H_t = np.hstack((np.identity(data1.shape[0]), np.zeros((data1.shape[0], samp_ann_params + 1  + 1))))\n",
    "    mu_bar = initial_ensembles.mean(0)\n",
    "    X_t, _,_, _, _, _, _ = forward_operation(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    X_t = X_t.transpose((0,2,1))\n",
    "    X_t = X_t.reshape(X_t.shape[0], X_t.shape[1]*X_t.shape[2])\n",
    "    script_H_t = np.kron(G_t.T, H_t)\n",
    "    G_u = (script_H_t@X_t.T)\n",
    "    G_u = G_u.T\n",
    "    G_bar = (G_u.mean(0)).ravel()\n",
    "    return mu_bar.reshape(-1,1), G_bar.reshape(-1,1), G_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7334c80c-cfcd-4d5b-b844-11cfb8c137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u): \n",
    "    u_j_minus_u_bar = initial_ensembles - mu_bar.reshape(1,-1)\n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    c = np.zeros((total_weights, G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        c += np.kron(u_j_minus_u_bar[i, :].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return c/size_ens, G_u_minus_G_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9148779c-1a3f-455d-a6ad-00fc1bf69c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_D_u( G_bar, G_u): \n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    d = np.zeros((G_bar.shape[0], G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        d += np.kron(G_u_minus_G_bar[i,:].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return d/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e8459b0-68c1-4d1f-851e-3d7b99f0ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_ensemble(data1, data2, data3, data4, initial_ensembles, y_train, size_ens = size_ens, inflation_factor = 1.0, fudging_beta = fudging_beta, \n",
    "                        fudging_var = None):\n",
    "    mu_bar, G_bar, G_u = calculate_mu_bar_G_bar(data1, data2, data3, data4, initial_ensembles, fudging_beta)\n",
    "    C, G_u_minus_G_bar = calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u)\n",
    "    D = calculate_D_u( G_bar, G_u)\n",
    "    _, R_t = create_cov(data1.shape[0],initial_ensembles)\n",
    "    inflation = np.identity(R_t.shape[0])*inflation_factor\n",
    "    D_plus_cov = D + (R_t *inflation_factor)\n",
    "    D_plus_cov_inv = np.linalg.inv(D_plus_cov)\n",
    "    mid_quant = C@D_plus_cov_inv\n",
    "    noise_vec_mean = np.zeros((R_t.shape[0], ))\n",
    "    noise_mvn = mvn(noise_vec_mean, R_t)\n",
    "    fudging = noise_mvn.rvs(size_ens)\n",
    "    interim = (y_train.T.flatten().reshape(1,-1) + fudging)\n",
    "    right_quant = interim - G_u\n",
    "    mid_times_right = mid_quant@right_quant.T\n",
    "    updated_ensemble = (initial_ensembles + mid_times_right.T)\n",
    "    if fudging_var is not None: \n",
    "        mean_vec = np.zeros((updated_ensemble.shape[1],))\n",
    "        cov_mat = np.identity(updated_ensemble.shape[1])*fudging_var\n",
    "        fudging_for_updated_ensembles = mvn(mean_vec, cov_mat)\n",
    "        fudging_for_updated_ensembles_vec = fudging_for_updated_ensembles.rvs(size_ens)\n",
    "        updated_ensemble = updated_ensemble + fudging_for_updated_ensembles_vec\n",
    "    return updated_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c14b236d-6ebf-40e3-9085-97afd2990e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "346d07bc-5b61-4e75-a816-a6e22ffe9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_D = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5abd35fb-ea92-449d-894a-162ed1f4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "381fc654-8877-41fd-840a-4b35153f923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cov(shape, initial_ensembles):\n",
    "    cov_part = initial_ensembles[:, -4:-3]\n",
    "    cov_part = cov_part.mean(0)\n",
    "    variances1 = tf.math.softplus(cov_part).numpy()\n",
    "    n = shape\n",
    "    return variances1, np.identity(n)*variances1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c59ea29-6484-40d5-8777-80cf9ca6cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7649cb4e-4909-404a-86db-cfb8fc2e078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data//gcn_cdr_train_pca.pickle\", \"rb\") as f: \n",
    "    catch_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f029a85a-6790-4635-b02f-a8dfd9346e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data//gcn_cdr_test_pca.pickle\", \"rb\") as f: \n",
    "    catch_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cbefaa8-c226-47fb-a972-1a8207d29766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "176445fb-4f30-421a-a710-339955111baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_train(catch_train, size): \n",
    "    idxes = random.sample(range(0, catch_train[0].shape[0]), k = size)\n",
    "    idxes = list(idxes)\n",
    "    data1, data2, data3, data4 = catch_train[0][idxes,:], catch_train[1][idxes,:], catch_train[2][idxes,:], catch_train[3][idxes,:]\n",
    "    \n",
    "    y_train = catch_train[-1][idxes].reshape(-1,1)\n",
    "    \n",
    "    return data1, data2, data3, data4, y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7c13b4e-c9b7-4b7f-bd6e-92ec93cd40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_test( catch_test, size): \n",
    "    idxes = random.sample(range(0, catch_test[0].shape[0]), k = size)\n",
    "    idxes = list(idxes)\n",
    "    data1, data2, data3, data4 = catch_test[0][idxes,:], catch_test[1][idxes,:], catch_test[2][idxes,:], catch_test[3][idxes,:]\n",
    "    y_train = catch_test[-1][idxes].reshape(-1,1)\n",
    "    return data1, data2, data3, data4, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2a4da04-1a68-4b91-ad62-e6c027c91fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1_train, data2_train, data3_train, data4_train, y_train =  prepare_data_train(catch_train, size = 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e54f80e-5b8d-4f46-a24a-2514a3683a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1_test, data2_test, data3_test, data4_test, y_test =  prepare_data_test(catch_test, size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aadbd46f-4890-40cc-8176-f292b9c45cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65bf21e9-92c6-4824-93c3-87274b072afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(idx, var_weights = 1.0, var_weight_weights = 4.0, var_D = 1, inflation_factor = 1.6, fudging_beta = beta(1,19), \n",
    "               fudging_var = 1e-3, epochs = 30):\n",
    "    \n",
    "    # smiles_feats_train, rdkit_feats_train, smiles_feats_valid, rdkit_feats_valid, y_train, y_train_actual, y_valid, y_valid_actual, initial_ensembles  = prepare_data(idx, var_weights = var_weights, var_weight_weights =var_weight_weights, var_L = var_L, var_D = var_D)\n",
    "    \n",
    "    \n",
    "    data1_train, data2_train, data3_train, data4_train, y_train =  prepare_data_train(catch_train, size = 2500)\n",
    "    \n",
    "    data1_test, data2_test, data3_test, data4_test, y_test =  prepare_data_test(catch_test, size = 1000)\n",
    "    \n",
    "    X_t, initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_D = get_initial_X_t(data1_train, data2_train, data3_train, data4_train,\n",
    "                                                                                                 size_ens = size_ens, var_weights = var_weights,\n",
    "                                                                                                var_weight_weights = var_weight_weights,\n",
    "                                                                                                     var_D = var_D)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    best_rmse_train = 100000\n",
    "    \n",
    "    for i in range(0,epochs):\n",
    "        print(\"epoch number is \" +str(i))\n",
    "\n",
    "        initial_ensembles = get_updated_ensemble(data1_train, data2_train, data3_train, data4_train, initial_ensembles, y_train, size_ens = size_ens,\n",
    "                                                 inflation_factor = inflation_factor, fudging_beta = fudging_beta, fudging_var = fudging_var)\n",
    "        \n",
    "        G_u_train, w1, w2, w3, w4 = get_predictions(data1_train, data2_train, data3_train, data4_train, initial_ensembles, fudging_beta)\n",
    "    \n",
    "        li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)    \n",
    "        ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)  \n",
    "    \n",
    "        width_train = ui_train - li_train\n",
    "        avg_width_train = width_train.mean(0)[0]\n",
    "    \n",
    "        ind_train = (y_train >= li_train) & (y_train <= ui_train)\n",
    "        coverage_train= ind_train.mean(0)[0]\n",
    "    \n",
    "        averaged_targets_train = G_u_train.mean(0).reshape(-1,1)\n",
    "        rmse_train = np.sqrt(((y_train -averaged_targets_train)**2).mean(0))[0]\n",
    "        \n",
    "        pearsonr_train = pearsonr(averaged_targets_train.reshape(averaged_targets_train.shape[0],), \n",
    "                                 y_train.reshape(y_train.shape[0],))\n",
    "        \n",
    "        r_train = pearsonr_train.statistic\n",
    "    \n",
    "        G_u_test, _, _, _, _ = get_predictions_test(data1_test, data2_test, data3_test, data4_test, initial_ensembles)\n",
    "    \n",
    "\n",
    "    \n",
    "        li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)     \n",
    "        ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)   \n",
    "    \n",
    "        width_test = ui_test - li_test\n",
    "        avg_width_test = width_test.mean(0)[0]\n",
    "    \n",
    "        ind_test = (y_test >= li_test) & (y_test <= ui_test)\n",
    "        coverage_test= ind_test.mean(0)[0]\n",
    "    \n",
    "        averaged_targets_test = G_u_test.mean(0).reshape(-1,1)\n",
    "        rmse_test = np.sqrt(((y_test -averaged_targets_test)**2).mean(0))[0]  \n",
    "        \n",
    "        pearsonr_test = pearsonr(averaged_targets_test.reshape(averaged_targets_test.shape[0],), \n",
    "                                 y_test.reshape(y_test.shape[0],))\n",
    "        \n",
    "        r_test = pearsonr_test.statistic\n",
    "\n",
    "        print(\"Training Coverage, Widths, RMSE, and Pearson R\")\n",
    "        print(coverage_train, avg_width_train, rmse_train, r_train)\n",
    "        print(\"Testing Coverage, Widths, RMSE, and Pearson R\")\n",
    "        print(coverage_test, avg_width_test, rmse_test, r_test)\n",
    "        # print(w1.mean(), w1.std())\n",
    "\n",
    "        if (rmse_train < best_rmse_train): \n",
    "            best_rmse_train = rmse_train\n",
    "            # print(\"went here\")\n",
    "            best_train_width_mean = avg_width_train.mean()\n",
    "            best_train_width = avg_width_train\n",
    "            # best_smiles_weight = w1.mean()\n",
    "            best_coverage_train = coverage_train\n",
    "            best_rmse_train = rmse_train\n",
    "            best_pearson_r = r_test\n",
    "            best_test_width = avg_width_test\n",
    "\n",
    "            best_coverage_test = coverage_test    \n",
    "            best_rmse_test = rmse_test\n",
    "            patience = 0\n",
    "            best_ensembles = initial_ensembles\n",
    "            \n",
    "        else:\n",
    "            patience = patience + 1\n",
    "            \n",
    "        print(\"Patience is\")\n",
    "        print(patience)\n",
    "        print('\\n')\n",
    "        \n",
    "        if (patience > threshold) | (i == (epochs-1)):\n",
    "            \n",
    "            # print()\n",
    "            # print(best_train_width.tolist(), best_coverage_train.tolist(), best_rmse_train.tolist(), best_test_width.tolist(), best_coverage_test.tolist(), best_rmse_test.tolist(), best_smiles_weight, flush = True)\n",
    "            # print(\"done for fold\" + str(idx), flush = True)\n",
    "            print(\"train_coverage\" + str(best_coverage_train), flush = True)\n",
    "            print(\"test_coverage\" + str(best_coverage_test), flush = True)\n",
    "            print(\"train_width\" + str(best_train_width), flush = True)\n",
    "            print(\"test_width\" + str(best_test_width), flush = True)\n",
    "            print(\"pearson\" + str(best_pearson_r), flush = True)\n",
    "            print(\"rmse_train\" + str(best_rmse_train), flush = True)\n",
    "            print(\"rmse_test\" + str(best_rmse_test), flush = True)\n",
    "            # print(\"smiles_weight_ci\" + str([best_li_smiles_weight, best_ui_smiles_weight]), flush = True)\n",
    "            \n",
    "            return [best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r, best_ensembles, [data1_test, data2_test, data3_test, data4_test, y_test]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11a8c623-2952-409f-a640-7d89cef6b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "909888fc-c2b2-49e9-b0f0-887c8fa4d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r, best_ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14658528-ea1a-4a6f-83bd-f708df0bcd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 153.68920744681475 6.84300624397454 0.09996166429965735\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 151.53985606263046 6.542772960175426 0.12736808645204506\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9848 58.872753741438174 10.575788369139087 0.0769794603429847\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.989 58.19755995731067 10.498630414710279 0.09354605004736455\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.5992 13.940588225857905 8.174516573019673 -0.1838008226961137\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.617 13.801283191948635 7.839107344510309 -0.13184609163939437\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.662 5.953231202846025 3.2400873125174945 0.33946588112647974\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.682 5.905339825957078 3.056730956664559 0.32735198324947085\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8508 5.568819224563978 1.8768408347649048 0.7728779680995189\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.861 5.503518157069559 1.8128539250050288 0.7384396462176336\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9008 5.315104783115281 1.5448128871327216 0.8536438608372154\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.889 5.265577338430224 1.5691658085547862 0.8184624999666269\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9024 5.113075700208157 1.4842154338993852 0.8648250537398606\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.879 5.065787883117844 1.5362413862604762 0.8241852383790621\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9088 5.196989794804063 1.4735921770429703 0.8667610266033248\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.894 5.150532442147049 1.5226024226991077 0.8263654930251751\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9044 5.195265609545914 1.4719663733666142 0.8672278367511547\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.883 5.136811345469758 1.5220248641496519 0.8256866812382391\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.914 5.293221432263095 1.471477860745121 0.8672676007217559\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.894 5.243190796070689 1.5213340689337782 0.8262800855505544\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9148 5.375110790781262 1.4731442245449216 0.86691067190426\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.891 5.304948270779374 1.5211978042606937 0.8277196047256182\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.91 5.170713205091901 1.4694474485377758 0.8675741580061449\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.886 5.131773368124615 1.5256044403673195 0.826188857092286\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9236 5.554003004634574 1.4714178289607922 0.8672044278140053\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.898 5.51060663438828 1.5267717210788514 0.826128401196551\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9072 5.120303500153283 1.4705679923652313 0.8673496251059898\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.881 5.069667308224303 1.5248235377692152 0.8262157388701084\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9152 5.368005401621638 1.4722757322004862 0.8670914862059168\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.896 5.30250985441616 1.5310670433123548 0.8255310756229837\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9116 5.3193186898495055 1.47363913894051 0.8667618698395709\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.896 5.2955436558492694 1.527446950483619 0.8256692989951\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9068 5.173032696324406 1.471658352914398 0.8671542837907603\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.887 5.121142904230645 1.526445056928415 0.8255295226581979\n",
      "Patience is\n",
      "5\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9132 5.342920309471353 1.4711693956721996 0.8673984025584868\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.891 5.298339330931842 1.5258793851174626 0.8268541600534923\n",
      "Patience is\n",
      "6\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.912 5.282860753880436 1.471808361260826 0.8672070124084753\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.891 5.247985342228325 1.5274890375020151 0.824725007285345\n",
      "Patience is\n",
      "7\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9136 5.329784085393182 1.4697850329244826 0.8675025215539044\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.898 5.276516560527469 1.5237767504321902 0.8265390711195713\n",
      "Patience is\n",
      "8\n",
      "\n",
      "\n",
      "train_coverage0.91\n",
      "test_coverage0.886\n",
      "train_width5.170713205091901\n",
      "test_width5.131773368124615\n",
      "pearson0.826188857092286\n",
      "rmse_train1.4694474485377758\n",
      "rmse_test1.5256044403673195\n",
      "CPU times: user 13min 33s, sys: 6min 35s, total: 20min 9s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r, best_ensembles, test_items = get_results(idx = 0, var_weights = 1,\n",
    "                                                                                                var_weight_weights = 4, var_D = 1, inflation_factor =1, fudging_beta = beta(1,19), \n",
    "           fudging_var = 1e-2, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06d8bd8b-05d0-458e-ac93-74d043a8562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_test, data2_test, data3_test, data4_test, y_test =  test_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9490af0c-5a1b-43fb-8431-b1282db34b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test, _, _, _, _ = get_predictions_test(data1_test, data2_test, data3_test, data4_test, best_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7779268c-ab61-4401-80a0-44538877c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_preds_test = preds_test.mean(0).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35f093e5-520c-41f0-a675-01ac46f87a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test, averaged_preds_test)\n",
    "plt.axline((0,0), slope = 1, c = \"black\")\n",
    "plt.xlabel(\"Ground Truth Values\")\n",
    "plt.ylabel(\"MEnKF-ANN Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b6dbeca-651e-4d31-89c0-c44e35b4270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4c969db3-06fd-4a30-8f3e-1d1670a03aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = pd.DataFrame(items).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01f44639-4484-45c7-bd7c-7745ab18371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.columns = ['best_train_width', 'best_coverage_train', 'best_rmse_train', 'best_test_width', 'best_coverage_test', 'best_rmse_test', 'best_pearson_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ab228b9-c61a-46cb-aa57-7acf4da3b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = items_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "925c745c-7a4d-4d78-8f47-9a1d7e11a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.columns = [\"Value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d93ed649-b0ba-445d-b28a-ffeb603dc960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>best_train_width</th>\n",
       "      <td>5.170713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_coverage_train</th>\n",
       "      <td>0.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_rmse_train</th>\n",
       "      <td>1.469447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_test_width</th>\n",
       "      <td>5.131773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_coverage_test</th>\n",
       "      <td>0.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_rmse_test</th>\n",
       "      <td>1.525604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_pearson_r</th>\n",
       "      <td>0.826189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Value\n",
       "best_train_width     5.170713\n",
       "best_coverage_train  0.910000\n",
       "best_rmse_train      1.469447\n",
       "best_test_width      5.131773\n",
       "best_coverage_test   0.886000\n",
       "best_rmse_test       1.525604\n",
       "best_pearson_r       0.826189"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c743cac6-4f17-49ab-9ad0-f35b685d22f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 1000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0a569-aed6-42db-afa8-83819760c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.sample(range(y_test.shape[0]), k = 16)\n",
    "fig, axs = plt.subplots(8, 2,figsize=(15, 15))\n",
    "axs = axs.ravel()\n",
    "for idx, i in enumerate(random_idx):\n",
    "    # print(counter)\n",
    "    truth = y_test[i,:]\n",
    "    preds = preds_test[:, i]\n",
    "    percts = np.percentile(preds, axis = 0, q = (2.5, 97.5))\n",
    "    lis = percts[0]\n",
    "    uis = percts[1]\n",
    "    \n",
    "    \n",
    "    axs[idx].hist(preds)\n",
    "    axs[idx].axvline(truth, color='green', linewidth=2)\n",
    "    axs[idx].axvline(lis, color='red', linewidth=2)\n",
    "    axs[idx].axvline(uis, color='red', linewidth=2)\n",
    "\n",
    "# plt.title\n",
    "fig.savefig('gcn_cdr_pred_intervals.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31471813-3497-4224-acf3-2bd150a35690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(y_train)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "26cbcc74-a96c-45c2-9e18-c15480f2be2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec9ecd4-18ce-4ef4-83c8-dded03b1f0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 164.1316848621056 7.666456453347576 -0.12910350367763337\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 164.6882169094109 7.593151383684547 -0.1382726170967484\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 69.65339942037564 11.338368325508187 0.48176711292245944\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 69.67060552280876 11.765265323830617 0.44302126050293533\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8216 22.058679378476054 8.087198933130292 0.16103798666705257\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.812 22.157106881115645 8.595136676438287 0.1444032282840821\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7676 9.448134438625068 4.082296803352457 0.31413138527650386\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.741 9.583796481308678 4.408515735095518 0.2644075806932144\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8364 5.678325923353889 2.0873395944879967 0.6907623814524825\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.807 5.7431721708960035 2.2793238257324644 0.6353382087491452\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8656 4.8624593661676405 1.5822345019620834 0.8267167457558773\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.845 4.9388147742559 1.7331197656179753 0.7887872256371574\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8904 4.957577381172942 1.4563342607885028 0.8555471237426825\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.882 5.0135665514532155 1.5914986059994762 0.8250176853447556\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8996 5.055042710140199 1.4385527770910977 0.859556871711958\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.883 5.107546547866001 1.5459513649640222 0.8351206480008072\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9116 5.166545440736957 1.4310807554468932 0.8608036352639932\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.887 5.217027770465368 1.5310639884479744 0.8387491865594655\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9112 5.26799363696276 1.4302972503353406 0.8609917127300414\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.897 5.315195425202388 1.534519201980459 0.8382939387236473\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.914 5.262336190838443 1.4257255420291082 0.861978355519006\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.892 5.28889979832864 1.5347811083202336 0.838149175348492\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9124 5.19196621846217 1.4263757526056824 0.8617890627556241\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.895 5.240142536851087 1.5290046987497024 0.8393864481277772\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9148 5.275871150930518 1.428928869759889 0.8612711976582489\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.905 5.346461927504877 1.5382007927757744 0.8374435705972572\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9132 5.181647600772834 1.4259074851839983 0.8619513895130284\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.899 5.250337780410458 1.525589262325587 0.839812070208506\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9104 5.2439937742548475 1.4271504598888671 0.8617292112921577\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.9 5.323252426712718 1.5300044062341316 0.8394864935775981\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.914 5.22514393477932 1.4240241153866562 0.862287107799433\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.896 5.3134801752332805 1.53338625757604 0.8384853172049422\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9108 5.184953039199197 1.426582861516404 0.8619943801535443\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.896 5.279742693886455 1.5398891932005943 0.8365983756388933\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9012 5.110826523385435 1.4250184821268392 0.8621035937103825\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.895 5.179030825624151 1.5298220045803494 0.8389607645360638\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9112 5.200312361036985 1.4236401039273556 0.862386082495478\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.897 5.270908991946134 1.5313529232416203 0.8389851740523703\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9132 5.256382975016717 1.425265634716515 0.862148848748628\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.898 5.3202483313267965 1.5354094469218782 0.8384199134567212\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "train_coverage0.9112\n",
      "test_coverage0.897\n",
      "train_width5.200312361036985\n",
      "test_width5.270908991946134\n",
      "pearson0.8389851740523703\n",
      "rmse_train1.4236401039273556\n",
      "rmse_test1.5313529232416203\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 171.25362287822236 8.032439377473729 0.17358687058529101\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 170.49798190061472 7.871729366961345 0.15626420965084456\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 51.22228717011125 7.887581461264868 0.3421735064073314\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 50.577757468296156 7.842325405370339 0.3370509104626139\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.89 16.145450297249557 4.931703441011542 0.3829608641299232\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.877 16.094545776401795 5.065539775210356 0.4346620571224785\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7676 6.058543223553707 2.572987215484332 0.6701481760667305\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.761 6.040633061267565 2.637950267359962 0.6853730036499848\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8648 4.780281130270011 1.6016874826325627 0.8224883850265593\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.848 4.757980107204985 1.6794244780068006 0.809980691540787\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8796 4.833813436118242 1.4950705860159685 0.8481453761803015\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.869 4.816095374816792 1.6111714267585495 0.8259885307129995\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8916 4.827185688557782 1.4582425639611143 0.8556403599996714\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.871 4.811021038805524 1.5878336981091548 0.8313216972746046\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8908 4.742996612426921 1.4399380127635442 0.8592968911882901\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.862 4.723646075803674 1.5945602651220327 0.829792812281205\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.892 4.802988920250148 1.4338109745668073 0.8605907693875666\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.859 4.772912459541617 1.5863080432611814 0.8324463324710323\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8832 4.661770412052243 1.4274436613239103 0.8618396175773128\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.85 4.629343493257559 1.585440410706115 0.8322088088808578\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8896 4.8152187513342755 1.4279523306019035 0.8618106887244185\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.867 4.784903362699742 1.586370797471957 0.8324611738046401\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8868 4.702555185811227 1.428027501871109 0.8618452582276327\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.853 4.677239732514852 1.581934080808305 0.8333748565836147\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8904 4.753798768171038 1.4267767687575066 0.8619920359505102\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.867 4.704523710159225 1.5846015236114612 0.8321889294498739\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.892 4.800710739852044 1.4282935837971005 0.8617058442212377\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.859 4.773178260668941 1.580060997783081 0.8332206762837079\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8848 4.624868064260094 1.4267707786701083 0.8620380323461205\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.849 4.598144403323851 1.5855384892949396 0.8321476521970418\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8876 4.719202176125603 1.426030401938376 0.8621268860524094\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.857 4.714520768816104 1.581724097331827 0.8329557111287935\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8876 4.669685612176449 1.4265044915284562 0.862088826202053\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.854 4.6274213904300305 1.5853018611830039 0.8325096271752929\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8844 4.652032998088553 1.4259978131362225 0.862176350542769\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.849 4.657328608492778 1.5844164929945181 0.8323008288880603\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.884 4.673761676889821 1.4267137913225845 0.8619886552400049\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.86 4.686698809013718 1.5858090982087674 0.8321499031356346\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8828 4.652646976702167 1.4267120185887208 0.8621032649525181\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.854 4.627386486932786 1.5811402073291547 0.8333536306139118\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "train_coverage0.8844\n",
      "test_coverage0.849\n",
      "train_width4.652032998088553\n",
      "test_width4.657328608492778\n",
      "pearson0.8323008288880603\n",
      "rmse_train1.4259978131362225\n",
      "rmse_test1.5844164929945181\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 149.73873122121216 6.616856773679286 0.2247482388780696\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 147.83391214836965 6.6781422520138465 0.2840542472722602\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 64.47019891657266 9.073315862056427 0.2982508127237587\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 64.4654155289262 8.677151889941289 0.29741272554510073\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8572 26.959728963888004 9.266429509187025 0.38235113116850306\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.859 26.987508116404328 9.316523797269467 0.4246005340151288\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.982 15.642038128908462 3.353351074105672 0.5184633654941444\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.986 15.642935868051262 3.315728424745764 0.5363041080774368\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.994 12.455612334532772 2.232669804400484 0.636647505918714\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.991 12.374378238952428 2.2993412847341186 0.6156583749226083\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9972 11.038393333228719 1.683465709756657 0.8020194633501663\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.995 10.990798055178367 1.7463615813251152 0.7879230668760457\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9972 11.165680588143246 1.5022477336254756 0.8493505248780799\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.993 11.12894806071423 1.5994380039630742 0.8277694840132378\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 11.09796789922133 1.4516077037275292 0.8569707579809893\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.994 11.118832634537082 1.5400205538941263 0.8404862742321351\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 11.042700200073671 1.4296162943555242 0.8615541643658802\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.993 11.030775642548168 1.5132590071847756 0.846054875242416\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9972 11.127224772821743 1.4325035957810277 0.8609858876818186\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.993 11.107197953679067 1.5249871986680743 0.843667423145209\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 10.866561325758955 1.4396696364488841 0.8595610597894506\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.994 10.804774424750132 1.5337720843617624 0.8413551939474491\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9972 11.043584334104036 1.434548917263548 0.8605440651602412\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.994 11.008737979768872 1.5243638122306926 0.8436242822138543\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9968 11.345763506771895 1.434457285139798 0.8605325057960898\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.993 11.272593732222838 1.530670266507459 0.8421775760857904\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 11.291885227233509 1.4268249157952602 0.8621529519337322\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.993 11.31401749120115 1.5342087688115242 0.8411980054213027\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9976 11.509600526797838 1.4395533862937446 0.8594996502145059\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.995 11.495657206052911 1.544880392792647 0.8390025113253704\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 11.261171158710415 1.4230319647756415 0.8630665482174592\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.994 11.202249562993623 1.526670386277898 0.8438093878124538\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9976 10.760338759204789 1.4257737152586716 0.8623938348385648\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.994 10.706791737439806 1.5332354590040456 0.8417781035311485\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 11.396387386595224 1.4396622957062462 0.8594594569683687\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.994 11.371602993598765 1.5451369614706183 0.8390001047813842\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 11.376506325306067 1.4219748808196353 0.8632975770077881\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.994 11.292356855076001 1.5159129893220098 0.8452480881869715\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 11.031867330023006 1.4223057110408799 0.863172983357372\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.995 10.972687005358075 1.5184356172360547 0.8455460746470745\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "train_coverage0.9984\n",
      "test_coverage0.994\n",
      "train_width11.376506325306067\n",
      "test_width11.292356855076001\n",
      "pearson0.8452480881869715\n",
      "rmse_train1.4219748808196353\n",
      "rmse_test1.5159129893220098\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 156.68694736522124 7.014611274998147 -0.02281112269728483\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 156.87387763089544 6.974778167710186 0.024115967744990384\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9916 49.485707800722146 9.66526201855284 0.18970067905736893\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.988 49.85284481667793 9.783134732765413 0.2219264093781751\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7148 13.409281616528515 6.2311315646272485 0.14965835643848796\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.712 13.434910126765315 6.27945840200854 0.17392785355183077\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7476 6.246963442278781 2.7226847194685777 0.4855804260135307\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.746 6.226390061101181 2.7446949910580782 0.4944897273132533\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8672 5.020697574732329 1.6956942126528987 0.8006120150832118\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.849 4.9849208377961025 1.7284346598663984 0.8000150976333454\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.92 5.401142261611174 1.4848331944358042 0.8502046411269002\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.9 5.37758317111457 1.5374115180606542 0.8451644473416624\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9184 5.402961476186571 1.4800220061064135 0.8513234804291694\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.9 5.378644033459832 1.527926124666497 0.8471539336534816\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9232 5.430923265000329 1.4559971961223857 0.8562484353043448\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.911 5.385204821166312 1.5111669363122835 0.850613529185844\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9168 5.390841232003168 1.4618945685739853 0.8551532431486922\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.911 5.372437919757729 1.5173939320347452 0.849414734577191\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9224 5.49579475813532 1.4686120776378448 0.8535196120788707\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.903 5.430855309421276 1.5298093464551314 0.8465761469331798\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9252 5.529939673572863 1.4539747657024844 0.8568738452052514\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.913 5.497166915296892 1.5032631240421073 0.8523054135835512\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9144 5.256456312172186 1.4491316190535997 0.8577199694757354\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.9 5.221202928812123 1.510474075585471 0.8508379460198227\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.92 5.416218870252887 1.4486566869171003 0.8578622012629814\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.91 5.383993705349115 1.5086256044694786 0.8512459419477146\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9264 5.582134409473625 1.4480692694167532 0.8581157922882285\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.916 5.554142369304866 1.502684772321213 0.8523998534117451\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9248 5.559969898107519 1.4477510104508335 0.8580351080951731\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.913 5.5294020349948765 1.5084415285650508 0.8511135588670183\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9184 5.371307329978403 1.448375780130728 0.8579872613035014\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.903 5.335785635491305 1.5035588675310334 0.8522012447452217\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9224 5.438035917040487 1.4495707431157618 0.8576478709598472\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.91 5.42252852482098 1.5043941036993256 0.8520269508448198\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9164 5.395518336292978 1.446373692562079 0.8586610303776572\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.907 5.363315900263877 1.499254666747553 0.8533062456639405\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9132 5.223503345992 1.4448783564792167 0.8587784921005501\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.901 5.18148529418508 1.4994066742832375 0.8530781506608167\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.918 5.32043999012543 1.446042378065625 0.858387373641819\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.906 5.284273710834719 1.5052717727026301 0.8518027359339121\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "train_coverage0.9132\n",
      "test_coverage0.901\n",
      "train_width5.223503345992\n",
      "test_width5.18148529418508\n",
      "pearson0.8530781506608167\n",
      "rmse_train1.4448783564792167\n",
      "rmse_test1.4994066742832375\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 168.7941299862124 9.565400001342407 0.05229097122578815\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 165.5732914623236 9.298697520746336 0.05834915484742553\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9788 51.544738107059835 10.975736155940263 0.0755571883565432\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.979 51.01688415314078 10.824204896457994 0.07183157938437648\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.6732 16.475203879372152 8.28105372573376 0.02727592037001322\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.66 16.300828293349394 8.253949746515119 0.04903975545462535\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7596 7.947651161147718 3.2405090997520736 0.43347169226935967\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.751 7.874368349004257 3.2746704000458275 0.42825675794210527\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.906 6.299555195255829 1.8521701859439461 0.7574200558609632\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.892 6.276909960183086 1.933822329019756 0.7441453963707904\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9336 5.983352030198273 1.5098851202468415 0.8437286270374798\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.927 5.980226075776219 1.5527935737625946 0.8396834748606677\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9416 6.0234406858349026 1.4570713265051618 0.8550143816127551\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.935 6.001489149551822 1.5074762753842406 0.8497634531968832\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.948 6.045780385031901 1.4463538787770396 0.8576832102023377\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.94 6.007396555418375 1.4915025669125026 0.8532325808476614\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9392 5.860989065652532 1.4364739683560999 0.859343714576778\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.929 5.817606234792776 1.4866608996912325 0.8539642141557945\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9412 5.870497244284829 1.4333319282263952 0.8599192864753121\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.932 5.845647760803627 1.4957712910702634 0.8519476440653619\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9404 5.770687414263332 1.4339221780663305 0.8597942743629412\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.93 5.756024418300789 1.4967482159325938 0.8517517169715254\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.936 5.782797336095218 1.4323336321459796 0.8602117826481249\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.924 5.765253882690557 1.4913789537690643 0.8529089502521646\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.94 5.82609820918699 1.4322225517849538 0.8601913248799995\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.927 5.786541017179302 1.4947772338516738 0.8522342424191309\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9384 5.723252146115424 1.4334543339937558 0.8601237394203236\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.927 5.713947675792146 1.4956677792515836 0.8521854991377802\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9348 5.7736276926955465 1.4334625854564687 0.859924474922274\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.924 5.720742453418397 1.4898978357879313 0.8533191886206571\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9368 5.676783690934531 1.4318034695150164 0.860318920910771\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.924 5.634577007357136 1.492330623643767 0.8528724021319495\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9424 5.845569437179908 1.4324571998763043 0.860179133670179\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.932 5.820435256313612 1.491274816100231 0.8529168464665254\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.936 5.743611069977972 1.431558393374047 0.8603006927366111\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.928 5.718748040618887 1.4892467114110897 0.8533377253695538\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9368 5.672160562945562 1.4303369831481887 0.8605851439260976\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.93 5.649945244886816 1.4925997977573076 0.8525859503634224\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9408 5.80744141072126 1.431374391408918 0.8603838316187478\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.919 5.79857330028688 1.4900635102648647 0.8531400460516163\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "train_coverage0.9368\n",
      "test_coverage0.93\n",
      "train_width5.672160562945562\n",
      "test_width5.649945244886816\n",
      "pearson0.8525859503634224\n",
      "rmse_train1.4303369831481887\n",
      "rmse_test1.4925997977573076\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 138.990645037071 8.850782228885313 0.06155704701829438\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 138.79946372642303 8.79518774697352 0.08430329361937984\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9732 92.91590725694518 22.389476109817227 -0.03295031780394071\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.973 93.1919359930766 22.393623268533165 -0.021654440804271858\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8628 37.002636344315405 12.50509804638713 0.04889783691131801\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.86 37.193320499968515 12.458054092413548 0.029806571666931174\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9256 19.57343982925135 5.229307343979703 0.1448716982098666\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.927 19.560448633151573 5.325721230493877 0.12030506076604834\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9952 15.805664884177066 2.489582287270335 0.5756753421491384\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 15.71989019838603 2.504511156244681 0.551428044582498\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 15.405592332939769 1.7799996873183404 0.7732284378836252\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 15.352203030616849 1.8414879614005544 0.747967886259462\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 14.403063646822341 1.697547599101664 0.7960952479853816\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 14.458703202926852 1.7567387391160196 0.7704228621502032\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9976 14.155142528317345 1.6958380541273184 0.7939842983131813\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 14.231428358907728 1.806690135342662 0.7533237332676149\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 13.84321782157693 1.6036870017980243 0.8188942281427553\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 13.939809617424388 1.709199731307951 0.7829018986431796\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9992 13.935265873879887 1.6098087242291224 0.8179271719263084\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 13.983709494948807 1.7209945354066478 0.7824457591508811\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9988 14.455137315772456 1.53999141226532 0.8335524511440578\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 14.462277938196497 1.6411675161775263 0.8023844587128093\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 14.019872329468216 1.5542161305078983 0.8316890180208973\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 14.069368857863441 1.6417772983281935 0.8026746008514855\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9988 13.966784480003948 1.5105794130475312 0.8404132382262901\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 13.989772711724054 1.6034143752574699 0.8116201150016268\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 13.49151525569241 1.4926767275072088 0.8451214391351034\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 13.503222372953337 1.6071252005945946 0.8111037394115265\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9988 13.8961511092184 1.45577786347056 0.8527456233138282\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 13.907688945821578 1.5492690040168393 0.825491777618563\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9992 14.04478824193365 1.457958600920055 0.8525030513486149\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 14.007905982767962 1.563639029567284 0.8227293484598366\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 13.781411287435892 1.478615928242824 0.849925829285327\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 13.804614150537773 1.572849877464958 0.8231404747766612\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 13.977637341720873 1.4505768199731288 0.8537942849729213\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 14.036551402092432 1.555089354880267 0.824418392364894\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9992 14.353186832224159 1.4488674374249206 0.8544667808357441\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 14.371114810396088 1.5624572464570112 0.8229517115543058\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 14.252935794594377 1.4303824325814352 0.8585057063798395\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 14.283982809296864 1.5435002350456948 0.8278936161777586\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "train_coverage0.9984\n",
      "test_coverage1.0\n",
      "train_width14.252935794594377\n",
      "test_width14.283982809296864\n",
      "pearson0.8278936161777586\n",
      "rmse_train1.4303824325814352\n",
      "rmse_test1.5435002350456948\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 107.00929426095588 5.792898991087573 0.046683489854062246\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 109.77054926293587 6.13992013742009 0.003962153212470536\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9008 52.44148177986066 14.558169957723647 0.34661496328820374\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.909 53.61920638338714 14.456383198206272 0.2682819737506017\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8864 19.110492846007755 5.569348375903747 0.2948532581509459\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.885 19.41047212819719 5.72938307936646 0.28349635538883233\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9364 9.332001035140113 2.4378625529006297 0.6333958656868139\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.943 9.481878752823167 2.511582827378503 0.6312116053292407\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.894 5.852007819809331 1.6848194050665268 0.8027135398309536\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.883 5.908599958975778 1.8055407559666206 0.7833435351701579\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8984 5.2060894655844105 1.4892545061566123 0.8508471575060043\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.889 5.232169178910945 1.6046848570348415 0.8336068003438818\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9196 5.5567292444641 1.4384797172816188 0.860358163525488\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.906 5.592500781056329 1.5675746283776453 0.8416644402098132\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.922 5.574938431735423 1.4357184073622404 0.8614745378161291\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.918 5.628588658986463 1.5640072672656724 0.8424938684960629\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9256 5.6318544909593475 1.429718294466616 0.8622349531639873\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.915 5.690072167723365 1.562881288032311 0.8425447245247485\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9292 5.680527157491814 1.4298547743725718 0.8621178748271945\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.919 5.738718793539414 1.5677722523130388 0.8416401520609893\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9252 5.636649925017866 1.429710632320408 0.8622276553058992\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.916 5.688309380459228 1.5650452630974678 0.8422472959403091\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9208 5.551370267880809 1.4268450584565513 0.8627695613839638\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.909 5.568383116802539 1.566730315099757 0.8421110397784448\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9152 5.4180141185204125 1.4261718107796757 0.8628760555248115\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.911 5.471494864897444 1.568369233022555 0.8415488986218518\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9164 5.5084502007974825 1.4271080590207306 0.8628606503731996\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.905 5.532549729237859 1.5648006011457156 0.8429185492414523\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9236 5.709351842585525 1.4266034427323249 0.8630872621766458\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.911 5.740673797063222 1.567765747357023 0.8427596373466409\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9196 5.527401862223561 1.4242398570880028 0.86333859542391\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.918 5.559949668348508 1.562836095548232 0.8428713099308281\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.928 5.696499178511877 1.423250977387275 0.8634819942090443\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.914 5.740165729190932 1.5599237985439027 0.8435048604388635\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9252 5.683374718275129 1.4247585546530936 0.8632653204638463\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.916 5.732937882656143 1.5609122136129332 0.843799607701572\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9192 5.5383476181029305 1.4269531471784833 0.8630860345322579\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.912 5.586609765412023 1.5695128098557585 0.8418166175187598\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.916 5.4176815096643605 1.4258119476198403 0.8631523823841916\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.909 5.477129207818561 1.5684572367909582 0.8425001653489494\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "train_coverage0.928\n",
      "test_coverage0.914\n",
      "train_width5.696499178511877\n",
      "test_width5.740165729190932\n",
      "pearson0.8435048604388635\n",
      "rmse_train1.423250977387275\n",
      "rmse_test1.5599237985439027\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 151.22498557581 8.958824623125142 0.28800253878575244\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 151.01857141425864 8.406576239175944 0.3009801309339276\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9832 57.91732876153696 11.141258340354563 -0.02591519883984296\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.979 57.85208421964945 11.364691367931007 -0.02536365962657559\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.828 13.04418314673773 4.880734689860899 0.17379720508849536\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.801 13.022189407883413 4.878455412129393 0.1747958885047557\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7604 6.110722961792494 2.6144936999108386 0.5257680361245725\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.763 6.145885636570473 2.586152709185554 0.5481505739659897\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8692 5.204742647650564 1.624121447052178 0.8325831083548433\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.894 5.2453405453124455 1.5884010508599824 0.8431116165400037\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.894 5.311221968969556 1.5042483356865015 0.8584103624698827\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.909 5.339357713960632 1.490506214576656 0.8632975542427624\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9096 5.578975384063022 1.4811952017886176 0.8631153145626419\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.922 5.606451367300981 1.474157166527841 0.866524578530698\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9176 5.658946714563464 1.4837859195612664 0.8627953911495995\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.923 5.643265745203481 1.4820186700656748 0.8651521897433124\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9176 5.622431814181808 1.472831001644148 0.8645959074072547\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.919 5.655723123457383 1.4709415444399028 0.867103381874257\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9232 5.777304334837683 1.4759511944553152 0.864030809080458\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.929 5.817260015011606 1.4647615965737986 0.8683136797573532\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.922 5.807148427026543 1.4724908162021342 0.8646872381706283\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.933 5.823664666209576 1.4602530538902703 0.8692252955603388\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9188 5.705410324668957 1.471309736920204 0.864980874227339\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.921 5.728976457697397 1.4680131632028681 0.8676673599040636\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.928 5.866302816648591 1.4760027109466614 0.8640562282376989\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.928 5.877489599288957 1.4679889495015968 0.8677294948548677\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.914 5.652310859867345 1.4707950173754454 0.8651417544023566\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.914 5.674139192200494 1.467907057859579 0.8678968005232661\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.91 5.571325890612295 1.4708720417195191 0.8650418118471872\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.909 5.573170674612347 1.4599943468049403 0.869231856103451\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9176 5.706116191490747 1.4701224612535473 0.8651875224122049\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.926 5.747045168205083 1.4683450774099518 0.8676914748123208\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9276 5.879380289935086 1.4705856727714408 0.8650897522120976\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.938 5.89891435205941 1.4638441654941958 0.8685111315067074\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9192 5.603342782227594 1.4698198336833512 0.865198803484486\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.919 5.6468770531124965 1.4649427093528582 0.868243322198839\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.932 5.892578941926957 1.4718090346526262 0.8648322167848187\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.939 5.912931626699098 1.4691338111456744 0.8675445129514482\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9236 5.821323911982999 1.4714616510889502 0.8649475313212602\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.926 5.83267196853766 1.4731585942197942 0.866682975269734\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "train_coverage0.9192\n",
      "test_coverage0.919\n",
      "train_width5.603342782227594\n",
      "test_width5.6468770531124965\n",
      "pearson0.868243322198839\n",
      "rmse_train1.4698198336833512\n",
      "rmse_test1.4649427093528582\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 194.80292428351498 9.914954128355625 0.1079346520021543\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 196.1575756619714 10.00697991062613 0.1154058858573787\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9352 146.96298309938953 38.27173025384244 0.040224881002555495\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.933 147.99866782957739 38.945734557059055 -0.0183553712415341\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7324 55.31519552360985 23.475333653144375 0.06845338934001102\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.738 55.35022165953263 23.413357121750654 0.02960240511187022\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.6564 24.41525698748367 12.175818190448313 0.319872829601562\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.667 24.782675358711813 12.176617015374982 0.28619863947830954\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8512 12.832274720258248 4.512177522258769 0.569673181271578\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.854 12.86618495271825 4.534432561708489 0.5553613040106558\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9736 9.3240218285851 2.0765141173640633 0.7553508056946494\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.969 9.400966238213933 2.0812279295152982 0.7573010773827584\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.98 8.200525855373513 1.5551153394426531 0.8415551748139516\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.977 8.256283770024082 1.5895370996600386 0.840074741181309\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9824 7.956733127720723 1.462044331616941 0.8563408774502727\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.982 7.986000018331415 1.4755748196824165 0.8539736994340097\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9816 8.091393090478581 1.4483714347785606 0.859359703357593\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.982 8.101599024606033 1.4697987725185637 0.8566765326819874\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.974 7.692968470143455 1.4432624806281489 0.8603862336727427\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.98 7.71648701651167 1.4534550561719208 0.8571640887507964\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9788 7.58708855512634 1.4408546852062676 0.8607999215778768\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.978 7.632451593939293 1.4615503864052852 0.8557747309793224\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9796 7.501518219237193 1.4377729083331912 0.861370641047251\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.974 7.560043839307264 1.4620287595362447 0.8563201147645357\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9752 7.488875601443628 1.438646715490686 0.8611362013606232\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.978 7.516227781224128 1.4550769615429093 0.8583940166442149\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9756 7.552487095144054 1.4347654018734928 0.8619217377365936\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.98 7.616369299661133 1.4518407694341255 0.8590224963310907\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.98 7.813610557933987 1.4366418708205984 0.8616014306264633\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.978 7.843153809434955 1.455292807116491 0.8573750324352879\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9844 8.030070478286431 1.4440812456699912 0.8606446619060714\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.978 8.082988281599068 1.4736916486909797 0.8572215468627862\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9848 8.127320781433827 1.4371636128232657 0.861570530720901\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.984 8.181345608941742 1.4545013654661465 0.8596912958164248\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.982 7.944675030618394 1.435903079151305 0.8617336121009661\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.979 7.946382503217157 1.4590717495656857 0.8586246216965113\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9828 8.042135139140367 1.4367041189911418 0.8616548828251005\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.981 8.095588883617719 1.4630906885104233 0.8576679299846854\n",
      "Patience is\n",
      "5\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9788 7.903610341676514 1.43718493499497 0.8615769446797383\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.98 7.934024829256454 1.4626179181231378 0.8585127226298258\n",
      "Patience is\n",
      "6\n",
      "\n",
      "\n",
      "train_coverage0.9756\n",
      "test_coverage0.98\n",
      "train_width7.552487095144054\n",
      "test_width7.616369299661133\n",
      "pearson0.8590224963310907\n",
      "rmse_train1.4347654018734928\n",
      "rmse_test1.4518407694341255\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 202.52198446078606 13.751513557825545 0.1167690733457061\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 202.32760720010066 14.218920028055134 0.11134821646012892\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.88 119.22595177780127 37.645255755640754 -0.042751435590072766\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.894 119.21553255970329 36.46814776014798 -0.023558663113748577\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.5148 32.07200785163662 27.255967801389012 0.3285717888126439\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.485 32.057410354647786 26.708829485195707 0.3280985140065632\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.552 13.601805863925478 9.890271878417453 0.4525855392767812\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.54 13.67899836977356 9.874351208346011 0.4534608105402981\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.838 9.4877014160485 3.426987185977382 0.6382530237992051\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.836 9.52603552323787 3.3967916413234223 0.644523307116722\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9708 8.283737640617238 1.6526417898692178 0.8236190892984488\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.984 8.355544453300205 1.5796957132697296 0.8371244217232208\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9772 8.155216880674075 1.4989406619599426 0.8510610053709241\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.986 8.151826092174883 1.449838379365321 0.8573757624302555\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9752 8.091044725719016 1.4888637664087525 0.8532884928245517\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.984 8.105671928821709 1.462933740048696 0.8545746424797611\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.972 7.831099732236865 1.4929922473340875 0.8523415434474435\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.976 7.880707307679025 1.4767678891492104 0.8520518087559509\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9756 7.968416984556267 1.4753913025951297 0.8561865734371888\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.981 8.00425768351496 1.4435770053754184 0.858693477077796\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9732 7.925223661010283 1.4739985415140087 0.8564579981506065\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.981 8.000138235371539 1.4437679623543997 0.8586223932800484\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9748 7.990587175309306 1.4694481152389405 0.8574460800465339\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.982 8.041911576597295 1.4411015478524927 0.8592617154843375\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9744 7.845929531563329 1.4691446405673707 0.8575176527482004\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.979 7.8919275870702785 1.4514578324349559 0.8578318405526539\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9736 7.911261400911078 1.4659374669791299 0.8580668932185549\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.98 7.945556892678519 1.4467949833785838 0.8581957107805714\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9736 7.938586000271898 1.4659549403091026 0.8580801157489016\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.983 7.993879529030459 1.4527134524432428 0.8567814507494066\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9792 8.13139336898338 1.4685255820806218 0.8575370319194617\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.985 8.211304136913913 1.4506527900877961 0.857366056945757\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9748 7.888910901051006 1.4693739133002452 0.857387165204044\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.98 7.931422504061694 1.4635069146975148 0.854785051234875\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9796 8.13586340399808 1.4648400401088355 0.8583527985701074\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.983 8.208974211731835 1.4565149874222587 0.8565595223340172\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.978 8.066015820240075 1.4696040797167442 0.8573269840976686\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.983 8.101810404038407 1.465131321187109 0.8547310312296414\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9796 8.1879739478311 1.4658635357822714 0.8581271004308643\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.984 8.22156568200887 1.4605933747754598 0.8554281437307486\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "train_coverage0.9796\n",
      "test_coverage0.983\n",
      "train_width8.13586340399808\n",
      "test_width8.208974211731835\n",
      "pearson0.8565595223340172\n",
      "rmse_train1.4648400401088355\n",
      "rmse_test1.4565149874222587\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 83.93655996617419 6.152923931525838 -0.25093744469653084\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 83.56540919787493 6.0181216521636305 -0.2624239279810013\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9624 27.897431983715492 6.299563747819747 -0.07130789299013199\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.957 27.344606927426458 6.3126928856400095 -0.14235797707473216\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.716 8.902996282546725 3.978880746357981 0.35918384624111804\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.717 8.768026806304821 4.007825014189208 0.3309188206837161\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7076 5.091857232564735 2.452427131215805 0.6035806281773899\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.701 5.027333165249973 2.4570110122489197 0.5845589805758614\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.79 5.0131999351262895 1.9376992665250004 0.7446579118502069\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.778 4.9770043662296715 1.9373542353088078 0.7335677292957521\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8904 5.022131925170259 1.514222211071098 0.8482946634603691\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.892 5.008457333872942 1.511935483776491 0.8418523546586623\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.888 4.795306671504291 1.4730847691744808 0.8571541179556227\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.885 4.7742584400021615 1.4654201091516668 0.8515432803749834\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8932 4.926453670216678 1.4694191879857452 0.857778872373538\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.896 4.9098815199225845 1.4661607588686125 0.8524145568434741\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8816 4.8482663969729 1.467551060436596 0.8582363708632427\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.896 4.833497697483218 1.4628942233205589 0.8524699296563746\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8832 4.79645293128996 1.4688873031390821 0.8578731456998342\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.878 4.758073999790199 1.4681253145115054 0.8516771748746584\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8768 4.691705890741055 1.4702674412280026 0.8577360875686443\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.884 4.646520602493534 1.4698474498131286 0.8516206423666544\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8764 4.656827981444245 1.4685301089205964 0.8579920811357014\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.88 4.654204353877805 1.4675376168982956 0.851482145749707\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8788 4.691572888479526 1.4695962681130421 0.8578386179782034\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.882 4.657796314159446 1.4685398317383158 0.8514684974698289\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8828 4.784398511972792 1.4676991690968435 0.8581338490838089\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.887 4.742795171636905 1.471357328360578 0.8508857520935035\n",
      "Patience is\n",
      "5\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8876 4.813123570988272 1.4658621467525252 0.858574754431102\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.889 4.791656905520356 1.4668888498971637 0.8522637603304931\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8872 4.821970199152407 1.4657775867279939 0.8585101389876237\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.885 4.7738070235556735 1.4626253429567622 0.8528648813590323\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.89 4.9216464928644 1.469127092546399 0.8578959501407867\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.901 4.899696080151613 1.4709113891028847 0.8515760670673495\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8776 4.618048803686623 1.4667276863851617 0.8583377329580352\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.88 4.590103063868337 1.461759769032466 0.8526950868553641\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8832 4.718908452890306 1.4654898157170833 0.858599552467115\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.885 4.702038808402749 1.4666115943045512 0.8516880836948049\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8932 4.975284700685296 1.4662876934646478 0.8585016938754787\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.894 4.950828495572118 1.4653673000515979 0.8519946733221399\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "train_coverage0.8832\n",
      "test_coverage0.885\n",
      "train_width4.718908452890306\n",
      "test_width4.702038808402749\n",
      "pearson0.8516880836948049\n",
      "rmse_train1.4654898157170833\n",
      "rmse_test1.4666115943045512\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 145.20598614888362 7.994033687550918 0.3135724996677122\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 141.87104477810155 7.763187330832909 0.31998280807889723\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9936 62.841317161909814 9.43161258494669 0.290058382418177\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.988 61.38356727401432 9.459306304540375 0.25322958886785424\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8692 19.419396010450168 6.1154461474595 0.0202278852659075\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.867 18.95536810293207 6.071744872357571 -0.04588799763044683\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7976 9.513738309920937 3.6387361063639836 0.4223292138441605\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.775 9.328407818430865 3.6174213751502085 0.36290495875689166\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9368 7.6090992351852185 1.926573971921576 0.7473455197866355\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.93 7.471578340573833 1.9489759015157517 0.7317503727873365\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9596 7.636654780844261 1.535626559494623 0.8428003978738533\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.956 7.516567259594643 1.6001496947057525 0.825025321105794\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9736 8.229706106150726 1.5190793726099552 0.8455325007085968\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.966 8.091238556926097 1.5823338347732028 0.8288781653659766\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9792 8.316418845375019 1.4883266704452682 0.853026672005116\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.968 8.17560147205012 1.5613066141117455 0.8339214204431373\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9792 8.289003376384256 1.4689075861025915 0.8569113454464378\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.976 8.167413429628876 1.5494448124492302 0.8377535771305972\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9824 8.67065711589793 1.4746426911796473 0.855188975389405\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.974 8.548229559493164 1.5558095380701573 0.8350386085389921\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9848 8.981168884185125 1.4741078904365665 0.8553397277918852\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.981 8.851574524556073 1.5555414854871716 0.83607436779517\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.982 8.967104759009043 1.4633178977997396 0.8576104309520531\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.975 8.862305385758253 1.5454866508042266 0.8377251307763648\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.982 8.814040613941499 1.4618942999310958 0.8580518256705394\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.974 8.655369420407801 1.5462462826574106 0.8372318125033332\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9864 9.306842339805943 1.465479116882725 0.8575830291715771\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.98 9.174255428110559 1.5544169114496778 0.8353636095990881\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9868 9.306891720753756 1.4626121915473387 0.8579167366532872\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.985 9.149425947704763 1.5531667365362478 0.8365739317688716\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9836 8.857869329062092 1.4653249519613354 0.8572496202509643\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.982 8.745494530607479 1.5602064622812655 0.8347658272443852\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9864 9.002803707322094 1.465599002936345 0.8577640168238282\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.978 8.818649709158034 1.5482396479117935 0.8386254865862919\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9848 8.76616292479553 1.4573725369022668 0.8588776549227377\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.979 8.64406789538415 1.5411943740374543 0.8389212104904199\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.986 8.852212774386949 1.4590145571429092 0.8589490462440363\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.973 8.666860482641667 1.548238961212746 0.8384339828112837\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9796 8.528597854141704 1.4813661794123751 0.8541152011447183\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.972 8.397199072278818 1.573852554509654 0.8307521061025759\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "train_coverage0.9848\n",
      "test_coverage0.979\n",
      "train_width8.76616292479553\n",
      "test_width8.64406789538415\n",
      "pearson0.8389212104904199\n",
      "rmse_train1.4573725369022668\n",
      "rmse_test1.5411943740374543\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 123.00082647072074 7.787808362881326 -0.1558655142183439\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 124.8346706756773 7.663089678318505 -0.15145965515647516\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.952 25.39097235004621 6.109482423927681 0.034951854952921274\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.951 25.613414683664203 6.403321400455925 -0.0002360683445840311\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.778 7.460479897012489 2.9033869879777234 0.5305633399147485\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.78 7.545650271964739 3.0268667244603993 0.5217125650873796\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7972 4.647231369667786 1.7529298245209097 0.7810302725149283\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.787 4.688091807907589 1.8293714058489274 0.7837015903940704\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8716 4.534955003569214 1.4824347701770149 0.8424703056639499\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.858 4.584968143762846 1.4925394629202828 0.8554169223445129\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8676 4.4760614520934485 1.4558425229915972 0.8481126224287368\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.855 4.522029010179474 1.4827102830904955 0.8563280032675227\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.866 4.482711160146537 1.4518486940274564 0.8490536274173258\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.85 4.516980958160257 1.4880717538951531 0.8549530366580513\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8608 4.477051044156638 1.4504334832307333 0.8493333896999671\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.859 4.528852408092832 1.4846926109089285 0.856042509729234\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.862 4.415450967626685 1.4504836973725082 0.8493618941720467\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.851 4.436250351531458 1.4856151810835347 0.8556968921688403\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8656 4.555128553455014 1.4488563178089833 0.8496938761162178\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.859 4.5916690395596795 1.4798127875114078 0.8571822352499392\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8672 4.468474822611896 1.4495663292712022 0.8496248876934301\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.858 4.5105339463515834 1.4845284501915608 0.856501684663647\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8692 4.492097629822716 1.448457782532701 0.8497841813162769\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.862 4.542024007353086 1.4793181534477178 0.8571556141119048\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8748 4.615751935114524 1.449556416144015 0.8496275515489619\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.864 4.644225767060385 1.4838703710439012 0.8557533937848233\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8792 4.640156095722144 1.4496507843344397 0.8497809123383012\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.866 4.668030627199116 1.4837156624218593 0.8556943433040521\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8692 4.5687318050805095 1.447177038559007 0.8500766193987921\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.856 4.604601243015383 1.4877452733658039 0.8556550719527378\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8712 4.530968910207315 1.4473629864303648 0.8500636984554075\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.867 4.571959009248312 1.4831676179586228 0.857017413112482\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8712 4.497185383924191 1.4476726467113457 0.8500343370512373\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.866 4.537003119410648 1.4866817447020582 0.8558423858294327\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8688 4.516440251230501 1.4483984420601648 0.849840726990663\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.861 4.560270248296853 1.485377312039603 0.8555811300904437\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.864 4.483122997840582 1.447366750178138 0.8500434818023249\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.849 4.522794793937539 1.484820635234755 0.8561233380986915\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8676 4.556547456568587 1.4473940841447466 0.850032653360858\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.857 4.591905589821102 1.485910243494341 0.8556761082345009\n",
      "Patience is\n",
      "5\n",
      "\n",
      "\n",
      "train_coverage0.8692\n",
      "test_coverage0.856\n",
      "train_width4.5687318050805095\n",
      "test_width4.604601243015383\n",
      "pearson0.8556550719527378\n",
      "rmse_train1.447177038559007\n",
      "rmse_test1.4877452733658039\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 173.07817455146682 10.182787504741167 0.07627378956486433\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 171.8148779768304 9.995161652572435 0.11952513983387106\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 53.22192886651879 6.553871882007819 0.2558282402542726\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 52.53670379186835 6.1463453606995895 0.27184294098061784\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.848 18.003278750472884 5.761849126258398 0.3413840496715466\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.869 17.88066162312761 5.48979962188145 0.4048839408682533\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.89 8.29227317142539 2.406531755869992 0.6893819128496418\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.901 8.174907527589108 2.347465432069197 0.7070087676003487\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9224 6.344994046931622 1.6692800803579835 0.8129346237659815\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.933 6.265838555893115 1.664485636150627 0.8184786783556086\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9268 5.795596529999384 1.5002192197288935 0.8385636643257425\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.927 5.704675474415508 1.4950281534118193 0.8429465619996585\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9252 5.631621874842469 1.4713464131472025 0.8448478092151102\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.925 5.536923687143444 1.4641520339452678 0.8495424269870914\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.92 5.488333220975065 1.469641218804394 0.8452367420461969\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.917 5.388686871390967 1.4590282436401767 0.8502503261422739\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9172 5.461181907031574 1.4632094093826278 0.8467177232257587\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.919 5.381080258023085 1.4680168313526707 0.8481792463120951\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9176 5.340704216889659 1.4641547712260417 0.8466832365520963\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.917 5.248136799049484 1.4715684957959163 0.8472402905351434\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9128 5.336863908104223 1.460428587419643 0.8474736451947698\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.918 5.254642946923149 1.4566183687152272 0.8509214808508467\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9152 5.365635683913469 1.4635318882462514 0.8465999779645964\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.918 5.282235746893708 1.4637119839908836 0.8491322494992317\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.91 5.32313339561393 1.459450370952744 0.8475282676591062\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.912 5.238559214818084 1.4598749328831284 0.8499444581377749\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.92 5.545415270967971 1.4603715335013436 0.8473382554278519\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.923 5.468556571897421 1.462159312439096 0.8494231533463144\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9188 5.360306031849763 1.4578329999212152 0.8479159250432151\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.92 5.286626005940744 1.459706543124869 0.8498152344397084\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9212 5.4125053866675295 1.4588642726246253 0.8476772141093096\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.918 5.334125737811475 1.4588285636124338 0.8502478572136113\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9248 5.57794499272407 1.4586766393117263 0.8478213447754519\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.924 5.487346531613835 1.4573896280244507 0.8505143462808152\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.924 5.547506900707461 1.4595317428925554 0.8475115694322594\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.925 5.486774614292803 1.464640943582589 0.8489097454010661\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9228 5.5793780689094445 1.4579005220484538 0.8478817176988541\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.927 5.495866649503453 1.463951733376914 0.8489644069250614\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9288 5.6495173475093345 1.4613229683053646 0.8473172295547495\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.928 5.565399553698352 1.4566894527539314 0.8505274671316871\n",
      "Patience is\n",
      "5\n",
      "\n",
      "\n",
      "train_coverage0.9188\n",
      "test_coverage0.92\n",
      "train_width5.360306031849763\n",
      "test_width5.286626005940744\n",
      "pearson0.8498152344397084\n",
      "rmse_train1.4578329999212152\n",
      "rmse_test1.459706543124869\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 108.62394470891478 6.63168847877035 -0.21873770551650798\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 107.51620393362283 6.661650719662115 -0.15607241105097092\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9912 67.0067707444116 12.810141296329324 0.23483412584712604\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.986 66.07108335479849 13.08617220230906 0.2196729308461099\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.732 31.480931090956926 14.346951488067868 -0.27012034324902084\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.735 31.194567716803505 13.433646762845457 -0.23991745210715196\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8428 17.915843440626915 6.214008140342884 0.2866531610810614\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.847 17.817543547291955 6.123229415386725 0.2675250806484126\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.984 15.595146556414983 3.224248150644746 0.4428369544382999\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.987 15.507891573395328 3.2341163956794015 0.4422077556276885\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9964 14.341261305171312 1.8367961573181268 0.7732912500328194\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 14.349480714437757 1.8475336978476842 0.7426913782572081\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9972 13.480516904046794 1.7200120768159648 0.8051774521989759\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.994 13.446568433950288 1.7192430044117546 0.7813097447275142\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 13.296140665947092 1.6374796198961443 0.8263747811827994\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 13.253784322323293 1.6477413939955292 0.8010141489571028\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 13.65827859368149 1.5913090416840585 0.8367124332070516\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 13.713496191746207 1.6175017987182023 0.8091328423961713\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9992 13.067390080275892 1.5413350398919368 0.846933953174027\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 13.068888300491773 1.5703884061821465 0.8220883099505679\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9992 12.887317943753791 1.544558899437936 0.8462242987451204\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 12.852297720032203 1.5357008809083423 0.8300109570974851\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 12.724538572696227 1.5059272002106692 0.8542894278505833\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 12.72327150235016 1.5260007449239967 0.8328616870825397\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 13.141060277381452 1.4986070459429932 0.8558022328066796\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 13.189435038191858 1.5380796618125545 0.830771130595136\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 12.940200067712743 1.4923466418228084 0.8571318175930698\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 12.909565029497756 1.5280933217518182 0.8337050095651525\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9992 12.934482756058326 1.4958854250051599 0.8574223733886859\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 12.922762824173565 1.540631165418888 0.8328104421040944\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 13.230699254391377 1.487407978721371 0.8581408919204888\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 13.170482196517634 1.5197663504656094 0.834492602272386\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 12.934177157099967 1.499120269107695 0.8557009389068675\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 12.889406640199928 1.5075323275597197 0.8373819282951477\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 12.830666038480578 1.4963362539212608 0.8570719820643115\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 12.756539440055748 1.5513285458615802 0.8301938694525122\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 12.818284774967326 1.4949563962349028 0.856891799910791\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 12.760698605516053 1.5357281800307685 0.8301203908124938\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9976 12.651043788559363 1.5019633117131428 0.8552918072282356\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 12.609413081512386 1.5365270531537305 0.8316606151106253\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "train_coverage0.9984\n",
      "test_coverage1.0\n",
      "train_width13.230699254391377\n",
      "test_width13.170482196517634\n",
      "pearson0.834492602272386\n",
      "rmse_train1.487407978721371\n",
      "rmse_test1.5197663504656094\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 131.4711051296504 8.448224547266246 -0.1049004333913562\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 130.03655933190768 8.57142697144583 -0.10502333319846573\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.978 64.5149889260944 13.25118947425666 0.3608690230955475\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.975 63.93044836370701 13.152672727251568 0.3408513733275928\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8308 28.42588316285253 10.147626718206412 0.11788385843214286\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.839 28.31244693888939 9.818745205389597 0.13609629576523677\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9332 17.552894003616213 4.694023222984355 0.45479741908063115\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.947 17.493051433163043 4.575060668184232 0.4731525944508287\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.994 13.72163427784404 2.056027548032643 0.7338851047718911\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.99 13.697733115243631 1.9785026393952916 0.7558147503464441\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.996 12.903684880957226 1.6956337473102387 0.8054305306016243\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.991 12.97347826090908 1.717726007667332 0.8000885933470658\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 14.335206016740948 1.5528584779806667 0.8401690167538007\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 14.328796369587339 1.6088275786656727 0.8272414074717922\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9976 13.83227687007125 1.502554849590356 0.8509254855530978\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 13.831357943723935 1.5839992535193357 0.8332534228670476\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 13.71890591122799 1.494951299821819 0.8525628984636863\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.995 13.640055734085134 1.5769965787601763 0.8358402471641263\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9976 13.36392305794103 1.473442677203855 0.8571947551267499\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.997 13.356424457575049 1.5598736012136423 0.8397565523027996\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9976 13.75180597608752 1.4862140643276291 0.8544174462207769\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 13.711225932467114 1.5887700164889156 0.8322615638510498\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9976 13.923809273189594 1.5387668220796085 0.8477168708498678\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 13.936267348663245 1.640837535557028 0.8284915532045101\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 14.143549682276763 1.4777861642039078 0.8564817588787119\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 14.116065191854878 1.5901248777543109 0.8316599344896685\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 14.176544348961427 1.4834098026549043 0.8557455297669423\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 14.18276654376882 1.5945863888628107 0.8305789086034062\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 14.330479319719522 1.4705462040159685 0.857797493699955\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 14.27578425642164 1.5673343533074364 0.8371140451827862\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9988 14.79094032848741 1.4780434552361335 0.8567054763725372\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 14.789432383878703 1.5974007645201822 0.8329810001985757\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 14.563553786279563 1.4890534316138977 0.8577218361609377\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 14.511813556185185 1.607201006724775 0.8360412346039225\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9976 14.414706987613464 1.463525680889756 0.8591647162603259\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 14.407428901734573 1.5820065840040989 0.8348111210954899\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 14.358977929727526 1.4873293442803261 0.8541647866360697\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.999 14.32306282640894 1.6017332940636404 0.8296823833780657\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9984 14.11146949749199 1.4613337941922875 0.8596138826942579\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.998 14.039718979460526 1.56968176683403 0.8368953124211459\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "train_coverage0.9984\n",
      "test_coverage0.998\n",
      "train_width14.11146949749199\n",
      "test_width14.039718979460526\n",
      "pearson0.8368953124211459\n",
      "rmse_train1.4613337941922875\n",
      "rmse_test1.56968176683403\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 151.3454503691052 7.030394844219004 -0.06459239241779735\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 150.3105668132579 7.203115347540899 -0.10235811473018605\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9188 61.164877901887145 17.513847801531945 0.1611320092353381\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.926 61.03549961681571 16.929207698894402 0.17861059149372738\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7416 14.36676027367483 6.520187884842984 0.1732934585939147\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.751 14.398829802332997 6.350580310618783 0.10881531623534042\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7568 6.727006860752961 2.8050135862584646 0.5472754806456175\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.74 6.731272073744514 2.9248314866854384 0.45103112504913273\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8924 5.373899178726793 1.6486884692869022 0.8118948689052286\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.862 5.400417889488348 1.7943133287855968 0.7546594293056855\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9088 5.14435628001889 1.4846576200202817 0.8493524872939221\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.878 5.17697130619972 1.6457260485631506 0.796910834654096\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.914 5.268188705500435 1.4476402461953206 0.8575445952919171\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.889 5.3027946327177755 1.6078199910005557 0.8075765478407907\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9188 5.284278668505502 1.4274962881766422 0.8617178182753409\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.898 5.3231448179998795 1.5863629390747642 0.8129467592600593\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.92 5.249129138642019 1.4216771758935698 0.8628317055680633\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.888 5.275106584524455 1.5917985569488997 0.8115037263546294\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9228 5.272060285385867 1.4199445181623092 0.8632572350422013\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.895 5.290151277111454 1.5856474240506666 0.8135070879465047\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.922 5.366131909168205 1.416463931345926 0.8638628470480093\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.899 5.4172106567890035 1.5843493209574475 0.813572490408375\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9252 5.3944711603445645 1.4159973523221483 0.8640357153602536\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.899 5.425289714017163 1.5915694014767459 0.8117762852246793\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9288 5.375927152519574 1.414582849523545 0.8642462893734665\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.897 5.376784951550815 1.5855660508577614 0.813281612668133\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9204 5.224360334655139 1.415356864210898 0.8640926967626162\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.893 5.242270025066922 1.5864612648878536 0.8130865521179538\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9216 5.309025347308977 1.4149168634344789 0.8642321098006339\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.886 5.330331470253888 1.588431255459905 0.8124486279049895\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9228 5.282826681031644 1.416956059739407 0.8638512406796062\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.889 5.2909472161627615 1.5840790378800367 0.8139189532404483\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9348 5.497325555081495 1.41554425077184 0.864039849576647\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.903 5.524210611539787 1.5988425048454626 0.8097296603994056\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9296 5.434011995706673 1.4152779940651143 0.8642971993982645\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.891 5.466676992270415 1.5890116776171759 0.8126923471477576\n",
      "Patience is\n",
      "5\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9324 5.436917517927822 1.4151991714530263 0.8641747353840656\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.894 5.465143885135307 1.58302444288986 0.8141370826330522\n",
      "Patience is\n",
      "6\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9276 5.356623037659685 1.4140463667647576 0.8643440347287401\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.894 5.3711798755513644 1.5846073368924762 0.8134354719041976\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "train_coverage0.9276\n",
      "test_coverage0.894\n",
      "train_width5.356623037659685\n",
      "test_width5.3711798755513644\n",
      "pearson0.8134354719041976\n",
      "rmse_train1.4140463667647576\n",
      "rmse_test1.5846073368924762\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 201.18809966625642 12.216917076415138 -0.3964779513168077\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 201.09755180199934 12.289195665250492 -0.39547690828721876\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.99 129.6624580725846 31.486307567171455 -0.08079203264447243\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.985 129.3140504539007 32.700795951553935 -0.04950277744313354\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7416 59.86189865126675 24.251832705835408 -0.16636558339984112\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.715 59.29804073080835 24.60727912512157 -0.14190738766312516\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9884 37.307605851901656 6.65823172227557 -0.002994627173298478\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.985 37.0908025278329 6.575909135515172 -1.1279678703700595e-05\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 38.9949220421773 2.9328634130961975 0.3617046125157321\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 38.67065774323273 2.9607441561351737 0.35088709639683274\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 40.29380415674016 2.040839050363343 0.7059724256403389\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 39.88471910927359 2.0623986449863243 0.7026152774022725\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 41.29261606012606 2.1850495583643537 0.6534488618649246\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 40.94588826816973 2.1911450055898563 0.651773851323232\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 40.563078790298185 1.976450103258664 0.727246598124038\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 40.1111083477026 2.0379053680793664 0.7082854561630875\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 40.88991273228775 2.1772441016656465 0.6497132481934177\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 40.743214910825145 2.2425695640803833 0.6268786474985698\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 39.76738083824116 1.9805577739761588 0.7221830141956792\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 39.4154211926803 2.0225146163506635 0.7112183099339703\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 38.01083599219953 2.1148172376579537 0.6741089640832977\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 37.816914335250054 2.1988190404730803 0.6453121707937222\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 40.696588714665026 1.9244680631061508 0.742555376486353\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 40.50623462512839 1.9615965088115423 0.7355119151437636\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 39.77736826044835 1.9745179482779207 0.7239891416139778\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 39.391624786243554 2.0152207647930735 0.7138413909026018\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 40.42856597899194 2.0784860805113854 0.7196894586618691\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 40.168473520306335 2.117209073038569 0.713935004730911\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 39.07946748886165 2.024959619489888 0.7074731448659606\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 38.86561683455449 2.080612168642317 0.6907638989162953\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 39.531786071447655 2.0280248319117433 0.7062502944443584\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 39.21671027168393 2.08086386134214 0.691398912402503\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 41.63880753530655 2.0308802648498645 0.705816245061115\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 41.40210451509434 2.072067678168755 0.6942800078489123\n",
      "Patience is\n",
      "5\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 41.71432003948209 2.0783248333723563 0.6988041252418933\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 41.402844691673465 2.1385180388713763 0.683533173728135\n",
      "Patience is\n",
      "6\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 39.062753179095466 1.7747791184192583 0.7876666267104383\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 38.57837639032879 1.8372041753565642 0.7712305777440448\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 40.31395979473322 2.1790381317112586 0.6851091640334273\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 39.89167893319414 2.248705620068573 0.670171378671508\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "train_coverage1.0\n",
      "test_coverage1.0\n",
      "train_width39.062753179095466\n",
      "test_width38.57837639032879\n",
      "pearson0.7712305777440448\n",
      "rmse_train1.7747791184192583\n",
      "rmse_test1.8372041753565642\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 226.59134451318812 12.040104820921226 0.05405532416376239\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 222.01072576377797 11.908046212172634 0.023165927010511924\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 155.77331161082 17.520811968229857 -0.08145665150872365\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 154.46728143932415 17.839099923311 -0.09506613263956695\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8656 88.6806182776494 29.774367494559307 -0.1553970989094441\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.864 86.85304509571485 29.360022702341904 -0.20448121883790082\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9576 49.41611147951987 13.834908150083901 -0.09035944975010585\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.951 48.58492406741367 13.67493854284821 -0.14438260067774447\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 43.19944947199865 4.562487325660548 0.36377721790361056\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 42.29077429132949 4.628888017842665 0.3141071789601362\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 38.38759381362571 2.4582797710071773 0.6782656840761992\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 37.75218402890774 2.4643752620239234 0.6690153919760434\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 37.12496354469387 1.8272818729789724 0.7628980500703012\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.433209620453994 1.8248234441781308 0.7579547146779292\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.78384350267088 1.5764009796446463 0.8292012392244432\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.02867715413471 1.62344245678307 0.814014207993984\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 37.21653785227268 1.5564818231494035 0.8361962070717768\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.45362237756197 1.5994054543185972 0.8217994235850775\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 37.819337673278255 1.5236656557105686 0.84038932264554\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 37.088635078280454 1.5644542947303428 0.8273508953271365\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.82459424206922 1.7281324257208106 0.7906272787119472\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.18636859341897 1.7556595435527313 0.7774017920991062\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 37.19568431747366 1.6823795616997685 0.8024172215536007\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.56245919108336 1.6599030772791439 0.8035169403141856\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.53394267951116 1.5191357875739229 0.8438286053186714\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 35.81073836139475 1.5555333703858745 0.8324292489695122\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 35.4698509900942 1.6186529365390845 0.8180693383708701\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 34.81857215332116 1.6560364076973269 0.8048663702053604\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.00772994795963 1.607824929130424 0.8255835285570496\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 35.1618654673478 1.623356162569548 0.8188553807324788\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 35.7571129832869 1.483516617082005 0.8516413507367457\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 35.12309215351426 1.525115982246489 0.8389227417688688\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.0597138634115 1.5646625626698536 0.8352963360157885\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 35.465799393202545 1.6168821856122222 0.8190357852854934\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.48624165359184 1.5611917867310037 0.8358055453926261\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 35.87960067340774 1.5799389820517837 0.8268116436473533\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 36.38923040606133 1.7174698067229242 0.80688402305815\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 35.665241679851135 1.7931004030536288 0.7840004796578665\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 35.613315824270785 1.5046713060416972 0.8456171104763435\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 35.02686090505471 1.5176707437712844 0.8394028246902321\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "train_coverage1.0\n",
      "test_coverage1.0\n",
      "train_width35.7571129832869\n",
      "test_width35.12309215351426\n",
      "pearson0.8389227417688688\n",
      "rmse_train1.483516617082005\n",
      "rmse_test1.525115982246489\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 151.76863083590302 5.412817560407387 0.0809239448159429\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 155.14564929723176 5.414222280857188 0.1356430349265289\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9664 93.19856273257008 20.64356222167539 -0.04498163131340948\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.965 94.14063736094494 20.10488471047252 -0.02458436704244559\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7812 48.09595979127954 21.30465067556046 0.08437693718341531\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.773 48.75495563776002 22.96132769057678 0.09908285681844702\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9028 23.959704443809716 7.138894321159956 0.4112395660133593\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.902 24.269572498756038 7.63562617983329 0.37680827515086057\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9968 20.462683869353747 2.945928145641609 0.6736891502489386\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 20.57005548105897 2.9810710600774906 0.663472818021555\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 19.970112858957197 1.7678838674979582 0.8121295946805676\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 20.177617672520675 1.8186704969484528 0.8215709312240287\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 19.59687730522448 1.6187353290986037 0.823783345920431\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 19.76482320234098 1.6368916661142947 0.8412955180285786\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 18.241551953502082 1.5354370212841717 0.8432442437947504\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 18.37132518563536 1.5515936201218479 0.8577924621753648\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 18.0431663610948 1.5654949351168377 0.8363273294239345\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 18.099968333610395 1.5826752672060072 0.8525669295243039\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 17.40191644060666 1.5184451952858584 0.8470899717929998\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.48378377531375 1.5702079087129446 0.8550335858466175\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 17.63096444795249 1.524250799442992 0.8462602849231567\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.681448976320503 1.5695555319921795 0.8558062671647163\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.43509528387483 1.5274342659847067 0.8450459849137527\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.535284120751058 1.5831483604350114 0.8515849455641928\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 17.770799189971004 1.5536377559323746 0.8394130727509967\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.829463818521894 1.5968759714443004 0.8496826105068414\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.472465874852112 1.509773703087476 0.8487512265172891\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.587881721450046 1.553883793250109 0.8573605744648133\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9992 17.55027000032093 1.5083370188607672 0.8490694103818323\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.634424351874785 1.543119379875437 0.8596331865682896\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9992 17.90449316794824 1.5458099286005997 0.840833298145092\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 18.014484360842136 1.589661639991516 0.849899474614052\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9996 17.28462969727638 1.4987748876066156 0.8510593076301664\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.400458458509426 1.560294239564975 0.8559788401467389\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 16.857294742658222 1.5090491684496714 0.8488482421824249\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 16.99722230165991 1.5722707163363423 0.8536067234069972\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9992 16.877333074130757 1.4986868981980381 0.8511960354035255\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.012510705824415 1.5387319450700847 0.8601468782421126\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.36726932385998 1.5013262227471011 0.8506834664718282\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 17.49774579073876 1.5455858799837334 0.859490221441258\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "train_coverage0.9992\n",
      "test_coverage1.0\n",
      "train_width16.877333074130757\n",
      "test_width17.012510705824415\n",
      "pearson0.8601468782421126\n",
      "rmse_train1.4986868981980381\n",
      "rmse_test1.5387319450700847\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 199.53913914755725 10.635108764755449 0.14870485437989087\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 197.75820355780118 10.90132871567272 0.17525034918764967\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9768 140.2204557822818 27.80711401591243 0.14808435171603498\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.967 139.00427196671743 27.776165624499573 0.2253951219823037\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.6584 66.30616199951659 34.52449888255178 -0.027629217109763706\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.664 65.30908772545177 34.649357860907664 0.02078994116476582\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9068 39.41376001340489 12.090783610988714 0.14134710653290605\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.905 38.50721835478613 12.307587141094563 0.16677781187652388\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 31.38361762565733 3.4375087908950697 0.564605332841813\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 30.71849088262607 3.304740403405813 0.5745407198682562\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 32.778990730050204 2.1701416143886565 0.7765617708854151\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 32.02484072089351 2.1223213601771462 0.7896317159202495\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 32.54078612962375 1.7559667781034298 0.8163779782098918\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 31.84466843935671 1.7344443052208298 0.8185412068450497\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 31.898268878298065 1.751199481940165 0.8028438376393724\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 31.09116903108326 1.6616748880848777 0.8233944823285329\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 32.56585430486169 1.5959788172637033 0.8308593790251835\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 31.777879912167613 1.6412476985450335 0.8198805999817825\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 31.782863693467604 1.6840642702275124 0.8168175843615605\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 30.98113080966593 1.6877155461630056 0.8141741013884114\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 31.54192486688887 1.5000206806738405 0.8486939676449152\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 30.676893109764812 1.531796390357441 0.8416494348358442\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 31.57475828868565 1.5708425024224064 0.8332697460931805\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 30.666645884797347 1.5823955836579167 0.8309280914112569\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 29.36599222689991 1.6231726767857964 0.8202675453046216\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 28.573455619446722 1.6254358575421568 0.8186173987903674\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 28.182667633131647 1.5923712509575436 0.8274096300891445\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 27.393271581141352 1.615092250656482 0.8215875317314011\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 28.303686644879786 1.5141741116617462 0.8458277320451266\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 27.642774845628832 1.5490972658354618 0.8366945684966691\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 27.404279545742273 1.5125981071882462 0.8460238303249528\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 26.697245462311926 1.5211992243581651 0.8432633790779521\n",
      "Patience is\n",
      "5\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 26.532808503799863 1.5961266048875593 0.829278040503432\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 25.865901108564376 1.6579014456782641 0.8138947370190849\n",
      "Patience is\n",
      "6\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 24.917892655095457 1.5239319488688594 0.8446945445519644\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 24.354015967107916 1.5797986294009456 0.8331541124675306\n",
      "Patience is\n",
      "7\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 24.84454014178718 1.4838681441429273 0.8520339271256218\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 24.298175489059687 1.516768487373874 0.844088101990161\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 24.99605654298794 1.5203835713081633 0.8441926942495488\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 24.475003204310923 1.542033182145898 0.8385568646475337\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "train_coverage1.0\n",
      "test_coverage1.0\n",
      "train_width24.84454014178718\n",
      "test_width24.298175489059687\n",
      "pearson0.844088101990161\n",
      "rmse_train1.4838681441429273\n",
      "rmse_test1.516768487373874\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 147.05383157231458 8.583429477917674 -0.009052977098518319\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 145.84610537983707 8.367436760936531 0.003443438499683196\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.986 66.83186407357519 12.387534857032565 -0.06191756976904431\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.992 66.62354626477483 12.004348189654706 -0.04685296287639649\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.6232 19.10413063966231 9.360406756888663 -0.24376772699013463\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.633 19.180940137440356 9.381530346971795 -0.2746930601227458\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7644 8.381900259253086 3.3156434461283855 0.2836594179063983\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.768 8.39356387265357 3.3263345717428168 0.2594845276018907\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8424 5.561530745554822 1.9239833761013896 0.7360581670316714\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.842 5.570805946022014 1.9109066539139339 0.7328014885765746\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9088 5.325103794687116 1.5265836616531767 0.8432951272048025\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.911 5.320488160617682 1.519741509131935 0.841841231202031\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9244 5.584544995139683 1.4755224187168172 0.8545294262702399\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.928 5.589081924049316 1.4846733634460616 0.8503071951805303\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9256 5.60260030184786 1.4606375455864067 0.8575619691912515\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.927 5.61891573703959 1.4542738379434474 0.8562435180224087\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.918 5.417355945662255 1.4572060850699688 0.8583231521374379\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.923 5.434401279675518 1.4648068217295873 0.8536166584389079\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.92 5.41343302220739 1.4558168330346717 0.8586369309774676\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.92 5.424749774908392 1.4619912489149713 0.8548310048861005\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9072 5.288207340333814 1.4567527763367256 0.8584070581382102\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.913 5.32044194931966 1.4741313339926765 0.8517948760784241\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9156 5.4080518714657035 1.455120877073003 0.8587380084068296\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.917 5.431860064896477 1.4693143499008512 0.8528897453045828\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9176 5.368555291709738 1.454919516792968 0.8589230727236816\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.917 5.382537209647652 1.467362079570099 0.8533822303923789\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9084 5.247143725342685 1.4548176270558033 0.8588306878809491\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.917 5.2663855655647955 1.4720385939105347 0.852546253424296\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9216 5.426558368971194 1.4539820979922726 0.8589582023774154\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.924 5.433407651035164 1.4671652906735317 0.8531897128194809\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9184 5.371833228699675 1.4546614557646596 0.8590384082351871\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.914 5.388793698745384 1.4666968943937955 0.8533319846333814\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9172 5.372704691896625 1.4534549176316243 0.8590478076186321\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.919 5.399012951285604 1.468430398201666 0.8530373566531195\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.926 5.521135056913699 1.4520029187289196 0.8593646193465331\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.922 5.543088885102335 1.4592410518867653 0.8552574031400323\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9188 5.3689061507261435 1.4535038312289594 0.8590762990881208\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.92 5.381276968956583 1.4700621030622167 0.8527706070800514\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9176 5.395404580544269 1.454781362128801 0.8589498798083536\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.918 5.393912182045103 1.4740756729168676 0.8523404619729836\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "train_coverage0.926\n",
      "test_coverage0.922\n",
      "train_width5.521135056913699\n",
      "test_width5.543088885102335\n",
      "pearson0.8552574031400323\n",
      "rmse_train1.4520029187289196\n",
      "rmse_test1.4592410518867653\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 152.97373673994295 9.202354683195543 -0.23079622684434442\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 152.60280355897086 9.767576265376738 -0.2691740110309847\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9556 47.9815789581463 11.398796200842732 -0.4150162906439194\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.955 48.08541369392402 11.67974958015956 -0.4384925556011332\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.6132 15.400448746414447 7.912775705940271 -0.36273246709690876\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.586 15.525631742891143 8.13798095047339 -0.375675048777958\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8536 13.192472920373937 4.081107094141975 0.3334158687760532\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.838 13.203916236900689 4.1737725315892265 0.3454970303833959\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9924 13.21897512244322 2.009103064484293 0.728580977684701\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.989 13.284936754154398 2.1868334734421544 0.6974892308394778\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.99 10.556691313957673 1.5862338708428176 0.826482425804471\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.984 10.581638902348333 1.7521199730067734 0.8038338524076556\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9892 10.199774706064773 1.5913627136995914 0.8249376905729304\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.98 10.20237357437699 1.7523358418413562 0.8034884080665753\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9924 10.931967285331028 1.4958477322659116 0.8483192957559003\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.985 10.966252344672313 1.6816120536982615 0.8217119626666433\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9924 10.495916526298583 1.480787533748942 0.8505277180054915\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.987 10.576995129137131 1.660079146988425 0.8256869816494262\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9936 10.304177416001163 1.4949546262716615 0.847730490539061\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.987 10.340088060687425 1.6709788163318473 0.8239201488592222\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9932 10.231427170922034 1.43865118955942 0.8595659921571945\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.983 10.221397105801517 1.6266280414592449 0.8334878538484368\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.994 10.515813569311655 1.4333097063420372 0.8612963443226883\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.985 10.485424711567765 1.6188490737204948 0.8358134128938133\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9948 10.518433704998836 1.4321183355975644 0.8609432980472992\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.987 10.544802245410532 1.6204738700846884 0.8349172293324556\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9932 10.36364147859183 1.437415512816292 0.8616468931678791\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.986 10.39783728035298 1.6184794941589618 0.8371066892173409\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9948 10.935325372011786 1.4272425988939539 0.8619426428778038\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.986 11.016291039224996 1.6128995216384956 0.8364914348035172\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9932 11.051564216489723 1.4268794586757518 0.86204417306334\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.987 11.050469725528899 1.6040408675023565 0.8384284308080105\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9948 10.968606737248898 1.4253278599030021 0.8624240680230586\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.985 10.99697071854291 1.6085971609062992 0.8374171633044225\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9964 10.99635521795738 1.4362511325647063 0.8604815227879536\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.988 11.000702085908651 1.606047652047095 0.8385487478770349\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9944 10.385045013534139 1.423930448750711 0.8627006331005789\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.983 10.422986579975774 1.614983869082861 0.8364292118127356\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9916 10.423948103994299 1.4270133984841213 0.8621079032136638\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.985 10.407854916039748 1.6018285262076446 0.8389164429240584\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "train_coverage0.9944\n",
      "test_coverage0.983\n",
      "train_width10.385045013534139\n",
      "test_width10.422986579975774\n",
      "pearson0.8364292118127356\n",
      "rmse_train1.423930448750711\n",
      "rmse_test1.614983869082861\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 138.91448496544476 12.42983731513265 -0.07956415457972553\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 137.947265234755 12.001429910648264 -0.06729965446670338\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8872 45.19900844675933 13.175079504210723 0.23234685379633419\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.875 45.15510940502415 13.496276381645796 0.3105922637939656\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.642 13.295283505953662 6.687014040762446 0.3498534267060478\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.646 13.266281888358884 6.985005754551256 0.400053526028349\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8304 7.559356638892796 2.753405064915152 0.5982062275141922\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.829 7.559185012884129 2.8075415229146965 0.6195143185942642\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8868 5.575042087800853 1.7399981745624944 0.7991805355484294\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.875 5.521836755650835 1.7699738968220922 0.800419912322566\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.894 5.001050344914758 1.5097837836697485 0.8484645651285359\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.886 4.95104218635136 1.5990413786719184 0.8344211128796064\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9048 5.083454216382307 1.4902797377907842 0.8530328960776629\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.893 5.039064265686904 1.5981040671386724 0.834696440930806\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8992 5.036541182517475 1.4858930046852579 0.8535174556984956\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.879 4.986975993227094 1.593294646469236 0.8362683366862409\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9008 5.085304331097996 1.4815507314839134 0.8545040415053688\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.884 5.037296339355232 1.5964703300155978 0.8365020292946083\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.896 5.111565112856663 1.4784194683827885 0.8551158379404001\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.886 5.060839562918009 1.5956032376895726 0.8364673467137224\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9104 5.179718757788083 1.4790540587319634 0.8549585921627704\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.887 5.119579760134655 1.6018816369217428 0.8343860326094036\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9008 5.052779417486742 1.4799147582636885 0.8547847071432298\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.89 5.012283732940545 1.6013469186508063 0.8346958050485138\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 12\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9016 5.085313177786031 1.478899993039531 0.8549950214712487\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.887 5.0303691286866545 1.5939112873173862 0.8364223906389695\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 13\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8916 5.024297822238034 1.4768687377193235 0.8554088482509657\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.877 4.950458502009378 1.5927338326710492 0.8363948838703295\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 14\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8924 4.964395656332057 1.4778053773643272 0.8554276554294034\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.88 4.934328146125353 1.5963673487062275 0.8352726626436741\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 15\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9028 5.117625153955218 1.480195130395013 0.8548112986221199\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.884 5.070183699184714 1.593817510922275 0.8357812759806452\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 16\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9004 5.0942007641740465 1.4782019020448345 0.8551877308589287\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.885 5.067175949012061 1.59603321159206 0.8362164211734074\n",
      "Patience is\n",
      "3\n",
      "\n",
      "\n",
      "epoch number is 17\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.912 5.13019822387909 1.4775717544765896 0.8552608787553329\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.897 5.100076478713481 1.5893267869129575 0.8372523773708785\n",
      "Patience is\n",
      "4\n",
      "\n",
      "\n",
      "epoch number is 18\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9032 5.048174978189982 1.4789658603314801 0.8549885726120886\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.885 5.003578463376814 1.5984345981609658 0.8349715749805579\n",
      "Patience is\n",
      "5\n",
      "\n",
      "\n",
      "epoch number is 19\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8988 5.039180852636942 1.4771742585188254 0.8553495225146369\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.885 4.981993575915622 1.5975302729010739 0.8355540564483358\n",
      "Patience is\n",
      "6\n",
      "\n",
      "\n",
      "train_coverage0.8916\n",
      "test_coverage0.877\n",
      "train_width5.024297822238034\n",
      "test_width4.950458502009378\n",
      "pearson0.8363948838703295\n",
      "rmse_train1.4768687377193235\n",
      "rmse_test1.5927338326710492\n",
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 130.34681159627112 5.045343796777787 0.17985238174224083\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 130.43918710235866 5.148315804704348 0.17090283097091535\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9888 44.03483144380281 8.26640391352806 0.20554339803691002\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.989 43.97218175376585 8.195841352363772 0.19626241701300753\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.6952 13.089065063954541 6.1399116047020845 0.15348733985055446\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.693 12.984738447741599 6.188292093844174 0.13778426393090923\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.7192 5.959028999969631 2.6865231398727367 0.557546020907893\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.737 5.8942502534584165 2.6863655603305987 0.5515574358626975\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.8528 5.513387925958599 1.7839696913068057 0.7835575876225767\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.86 5.4211163200418175 1.7258145792552142 0.7805626801518318\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.898 5.578849407308143 1.5657687157012699 0.8402304647517461\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.907 5.454097396145391 1.5273447960103899 0.8331570798481448\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9052 5.42326699922914 1.4918161284028124 0.8543040457818704\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.904 5.315384272756305 1.5023557187315044 0.8395950989847565\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n"
     ]
    }
   ],
   "source": [
    "catch_overall = []\n",
    "\n",
    "for idx in range(0,50): \n",
    "    best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r, _, _ = get_results(idx, var_weights = 1.0, var_weight_weights = 1.0, var_D = 1, inflation_factor =1, fudging_beta = beta(1,19), \n",
    "           fudging_var = 1e-2, epochs = 20)\n",
    "    \n",
    "    catch_overall.append([best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538d375-2e01-411d-a7fb-a9781b542b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data//catch_overall_50_reps.pickle\", \"wb\") as f: \n",
    "    pickle.dump(catch_overall,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9a198-1159-46a3-97bd-c4fa0550438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df = pd.DataFrame(catch_overall).iloc[:,:7]\n",
    "overall_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044d956-3e5a-4e5b-9ac3-54bb43a992f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df.columns = ['best_train_width', 'best_coverage_train', 'best_rmse_train',\n",
    "                      'best_test_width', 'best_coverage_test', 'best_rmse_test', 'best_pearson_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe6a4f-4552-4405-ba9b-4b2e62c578e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
