{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53e19a-64db-4b1d-9378-c19c8efacd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import block_diag\n",
    "import warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6028973-9799-4d8a-b919-a8dd6283faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34242ac3-a983-4bab-b092-bfcea4f94a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(weights_ann_1[0].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459b853-f520-4181-a9a9-310a6dd20de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, initial_ensembles, size_ens): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    # weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    # h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66ee4c-4112-4dca-a9cc-68a60d4e316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 32, input_shape = 256, output_shape = 1): \n",
    "    input_layer = tf.keras.layers.Input(shape = (input_shape))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(output_shape, activation = \"relu\")\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9ef03-af61-4ce4-99ff-2f0b5dca639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_ensembles(num_weights, lambda1, size_ens):\n",
    "    mean_vec = np.zeros((num_weights,))\n",
    "    cov_matrix = lambda1*np.identity(num_weights)\n",
    "    mvn_samp = mvn(mean_vec, cov_matrix)\n",
    "    return mvn_samp.rvs(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ea2b5-13a4-41c4-8257-c6c18708501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expit(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "#     e_x = np.exp(x - np.max(x))\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ac30a-bcc7-4b5e-9314-7fb85f284f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann =  ann(hidden = 16, input_shape = 32, output_shape = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2548f2de-a4c3-4850-b5cc-439634f5c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ann_1 = samp_ann.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338dc89-5540-4867-95a6-3d0ea7639cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1  = samp_ann.layers[1].output.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92b1e3-d9c6-4ce7-959d-aef1376a29d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b0370-90df-4ab4-85b9-691d082750bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06982568-e5c1-4712-895c-7b3d49a09040",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c4646-23a6-45ca-9ccc-aa4c3566d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann_params = samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f741d7-ba99-4803-8441-5bf2ad3d72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_X_t(data1, data2, data3, data4, size_ens, var_weights = 1.0, var_weight_weights = 4.0, var_L = 1.0, var_D = 1.0):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    \n",
    "    initial_ensembles1 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out1, data1_stack1 = get_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles2 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out2, data1_stack2 = get_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles3 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out1, data2_stack1 = get_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles4 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out2, data2_stack2 = get_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles_for_weights = generate_initial_ensembles(4, var_weight_weights, size_ens)\n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    # initial_ensembles_for_L = generate_initial_ensembles(4, var_L, size_ens)\n",
    "    # initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)    \n",
    "    \n",
    "    initial_ensembles_for_D1 = generate_initial_ensembles(1, var_D, size_ens).reshape(-1,1)\n",
    "    # initial_ensembles_for_D2 = generate_initial_ensembles(1, var_D, size_ens).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D1_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    initial_ensembles_for_D2_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D3_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.concatenate((np.expand_dims(initial_ensembles_for_D1,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D1_zero,1), \n",
    "                                                      np.expand_dims(initial_ensembles_for_D2_zero,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D3_zero,1)), axis = 2)\n",
    "    \n",
    "    # print(X_t.shape, initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4))\n",
    "    \n",
    "    return X_t, initial_ensembles, initial_ensembles_for_weights[:,0,:], initial_ensembles_for_D[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac49c8c-6da0-4ee2-9561-e1d19365f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_targets_with_weights(batch_data, initial_ensembles, size_ens, weights): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    final_output_1 = final_output_1*weights\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2f9ca-f4b4-445a-ab5f-e0f0557f54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed09c8-f6b0-4b40-86e0-1975541fd0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fudging_beta = beta(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1a4f5e-7a0d-42b7-ad45-59c708b7b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation(data1, data2, data3, data4, combined_ensembles , size_ens, fudging_beta):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    # initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4):(4*params + 4 + 4 )]\n",
    "    \n",
    "    \n",
    "    softmax_weights = softmax(initial_ensembles_for_weights)\n",
    "    \n",
    "    model_1 = softmax_weights[:, 0].reshape(-1,1) \n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 1].reshape(-1,1) \n",
    "    \n",
    "    model_3 = softmax_weights[:, 2].reshape(-1,1) \n",
    "    \n",
    "    model_4 = softmax_weights[:, 3].reshape(-1,1) \n",
    "    \n",
    "    sum_weights = model_1 + model_2 + model_3 + model_4\n",
    "    \n",
    "    \n",
    "    # model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "    model_1 = model_1/sum_weights\n",
    "    \n",
    "    model_2 = model_2/sum_weights\n",
    "    \n",
    "    model_3 = model_3/sum_weights\n",
    "    \n",
    "    model_4 = model_4/sum_weights\n",
    "    \n",
    "    \n",
    "    # print(np.mean(model_1 + model_2))\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_2)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_3)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_4)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    # initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    final_output = data1_out1 + data1_out2 + data2_out1 + data2_out2\n",
    "    \n",
    "    # weighted_psa = data1_out2 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles,final_output, model_1, model_2, model_3, model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd2eb3-86b7-41ed-81b4-4dd8e44e106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation_test(data1, data2, data3, data4, combined_ensembles , size_ens):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    # initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    \n",
    "    softmax_weights = softmax(initial_ensembles_for_weights)\n",
    "    \n",
    "    model_1 = softmax_weights[:, :1].reshape(-1,1) \n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 1:2].reshape(-1,1)\n",
    "    \n",
    "    model_3 = softmax_weights[:, 2:3].reshape(-1,1)\n",
    "    \n",
    "    model_4 = softmax_weights[:, 3:4].reshape(-1,1)\n",
    "    \n",
    "    sum_weights = model_1 + model_2 + model_3 + model_4\n",
    "    \n",
    "    \n",
    "    # model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "    model_1 = model_1/sum_weights\n",
    "    \n",
    "    model_2 = model_2/sum_weights\n",
    "    \n",
    "    model_3 = model_3/sum_weights\n",
    "    \n",
    "    model_4 = model_4/sum_weights\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_2)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_3)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_4)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    # initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    final_output = data1_out1 + data1_out2 + data2_out1 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles, final_output, model_1, model_2, model_3, model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3dd570-7ab3-4eae-bd8e-9a57a333b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = 4*(samp_ann.count_params() + 1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820b010-4396-4b7a-8781-a8e6d503aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbaaf7-afae-4008-a26c-e0691ef9b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = total_weights//reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5111536-1a97-4390-9658-c9848a136a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f598992-73d6-4b5a-906d-3184f1b9625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_t = [[1, 1, 1, 1]]\n",
    "G_t = np.array(G_t).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef8c41-26ff-4e49-98f8-8f6c31b4eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84cb0e3-2f5e-4207-b092-d2768b52205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data1, data2, data3, data4, initial_ensembles, fudging_beta  =fudging_beta): \n",
    "    _,_, weighted_alogp, w1, w2, w3, w4 = forward_operation(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    return weighted_alogp, w1, w2, w3, w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b60b37-92b1-4784-85fc-59a865d2398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_test(data1, data2, data3, data4, initial_ensembles): \n",
    "    _,_, weighted_alogp, w1, w2, w3, w4 = forward_operation_test(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens)\n",
    "    return weighted_alogp, w1, w2, w3, w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0a9b9-6202-40e2-911f-3c890213099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mu_bar_G_bar(data1, data2, data3, data4, initial_ensembles, fudging_beta):\n",
    "    H_t = np.hstack((np.identity(data1.shape[0]), np.zeros((data1.shape[0], samp_ann_params + 1  + 1))))\n",
    "    mu_bar = initial_ensembles.mean(0)\n",
    "    X_t, _,_, _, _, _, _ = forward_operation(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    X_t = X_t.transpose((0,2,1))\n",
    "    X_t = X_t.reshape(X_t.shape[0], X_t.shape[1]*X_t.shape[2])\n",
    "    script_H_t = np.kron(G_t.T, H_t)\n",
    "    G_u = (script_H_t@X_t.T)\n",
    "    G_u = G_u.T\n",
    "    G_bar = (G_u.mean(0)).ravel()\n",
    "    return mu_bar.reshape(-1,1), G_bar.reshape(-1,1), G_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334c80c-cfcd-4d5b-b844-11cfb8c137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u): \n",
    "    u_j_minus_u_bar = initial_ensembles - mu_bar.reshape(1,-1)\n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    c = np.zeros((total_weights, G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        c += np.kron(u_j_minus_u_bar[i, :].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return c/size_ens, G_u_minus_G_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148779c-1a3f-455d-a6ad-00fc1bf69c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_D_u( G_bar, G_u): \n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    d = np.zeros((G_bar.shape[0], G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        d += np.kron(G_u_minus_G_bar[i,:].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return d/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8459b0-68c1-4d1f-851e-3d7b99f0ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_ensemble(data1, data2, data3, data4, initial_ensembles, y_train, size_ens = size_ens, inflation_factor = 1.0, fudging_beta = fudging_beta, \n",
    "                        fudging_var = None):\n",
    "    mu_bar, G_bar, G_u = calculate_mu_bar_G_bar(data1, data2, data3, data4, initial_ensembles, fudging_beta)\n",
    "    C, G_u_minus_G_bar = calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u)\n",
    "    D = calculate_D_u( G_bar, G_u)\n",
    "    _, R_t = create_cov(data1.shape[0],initial_ensembles)\n",
    "    inflation = np.identity(R_t.shape[0])*inflation_factor\n",
    "    D_plus_cov = D + (R_t *inflation_factor)\n",
    "    D_plus_cov_inv = np.linalg.inv(D_plus_cov)\n",
    "    mid_quant = C@D_plus_cov_inv\n",
    "    noise_vec_mean = np.zeros((R_t.shape[0], ))\n",
    "    noise_mvn = mvn(noise_vec_mean, R_t)\n",
    "    fudging = noise_mvn.rvs(size_ens)\n",
    "    interim = (y_train.T.flatten().reshape(1,-1) + fudging)\n",
    "    right_quant = interim - G_u\n",
    "    mid_times_right = mid_quant@right_quant.T\n",
    "    updated_ensemble = (initial_ensembles + mid_times_right.T)\n",
    "    if fudging_var is not None: \n",
    "        mean_vec = np.zeros((updated_ensemble.shape[1],))\n",
    "        cov_mat = np.identity(updated_ensemble.shape[1])*fudging_var\n",
    "        fudging_for_updated_ensembles = mvn(mean_vec, cov_mat)\n",
    "        fudging_for_updated_ensembles_vec = fudging_for_updated_ensembles.rvs(size_ens)\n",
    "        updated_ensemble = updated_ensemble + fudging_for_updated_ensembles_vec\n",
    "    return updated_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b236d-6ebf-40e3-9085-97afd2990e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d07bc-5b61-4e75-a816-a6e22ffe9d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_D = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd35fb-ea92-449d-894a-162ed1f4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b589c-d023-4fdb-8d25-0edd1fea0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return np.log(np.exp(x) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381fc654-8877-41fd-840a-4b35153f923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cov(shape, initial_ensembles):\n",
    "    cov_part = initial_ensembles[:, -4:-3]\n",
    "    cov_part = cov_part.mean(0)\n",
    "    variances1 = softplus(cov_part)\n",
    "    n = shape\n",
    "    return variances1, np.identity(n)*variances1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59ea29-6484-40d5-8777-80cf9ca6cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7649cb4e-4909-404a-86db-cfb8fc2e078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data//gcn_cdr_train_pca_activations.pickle\", \"rb\") as f: \n",
    "    catch_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f029a85a-6790-4635-b02f-a8dfd9346e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data//gcn_cdr_test_pca_activations.pickle\", \"rb\") as f: \n",
    "    catch_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbefaa8-c226-47fb-a972-1a8207d29766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176445fb-4f30-421a-a710-339955111baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_train(catch_train, size): \n",
    "    idxes = random.sample(range(0, catch_train[0].shape[0]), k = size)\n",
    "    idxes = list(idxes)\n",
    "    data1, data2, data3, data4 = catch_train[0][idxes,:], catch_train[1][idxes,:], catch_train[2][idxes,:], catch_train[3][idxes,:]\n",
    "    \n",
    "    y_train = catch_train[-1][idxes].reshape(-1,1)\n",
    "    \n",
    "    return data1, data2, data3, data4, y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c13b4e-c9b7-4b7f-bd6e-92ec93cd40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_test( catch_test, size): \n",
    "    idxes = random.sample(range(0, catch_test[0].shape[0]), k = size)\n",
    "    idxes = list(idxes)\n",
    "    data1, data2, data3, data4 = catch_test[0][idxes,:], catch_test[1][idxes,:], catch_test[2][idxes,:], catch_test[3][idxes,:]\n",
    "    y_train = catch_test[-1][idxes].reshape(-1,1)\n",
    "    return data1, data2, data3, data4, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e54f80e-5b8d-4f46-a24a-2514a3683a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_test, data2_test, data3_test, data4_test, y_test =  prepare_data_test(catch_test, size = catch_test[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadbd46f-4890-40cc-8176-f292b9c45cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf21e9-92c6-4824-93c3-87274b072afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(idx, var_weights = 1.0, var_weight_weights = 1.0, var_D = 0.01, inflation_factor = 1.6, fudging_beta = beta(1,19), \n",
    "               fudging_var = 1e-3):\n",
    "    \n",
    "    # smiles_feats_train, rdkit_feats_train, smiles_feats_valid, rdkit_feats_valid, y_train, y_train_actual, y_valid, y_valid_actual, initial_ensembles  = prepare_data(idx, var_weights = var_weights, var_weight_weights =var_weight_weights, var_L = var_L, var_D = var_D)\n",
    "    \n",
    "    \n",
    "    data1_train, data2_train, data3_train, data4_train, y_train =  prepare_data_train(catch_train, size = 2500)\n",
    "    \n",
    "    \n",
    "    X_t, initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_D = get_initial_X_t(data1_train, data2_train, data3_train, data4_train,\n",
    "                                                                                                 size_ens = size_ens,\n",
    "                                                                                                     var_weights = var_weights,\n",
    "                                                                                                var_weight_weights = var_weight_weights,\n",
    "                                                                                                     var_D = var_D)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    best_rmse_train = 100000\n",
    "    \n",
    "    for i in range(0,10000):\n",
    "        # print(\"epoch number is \" +str(i))\n",
    "\n",
    "        initial_ensembles = get_updated_ensemble(data1_train, data2_train, data3_train, data4_train, initial_ensembles, y_train, size_ens = size_ens,\n",
    "                                                 inflation_factor = inflation_factor, fudging_beta = fudging_beta, fudging_var = fudging_var)\n",
    "        \n",
    "        G_u_train, w1, w2, w3, w4 = get_predictions(data1_train, data2_train, data3_train, data4_train, initial_ensembles, fudging_beta)\n",
    "    \n",
    "        li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)    \n",
    "        ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)  \n",
    "    \n",
    "        width_train = ui_train - li_train\n",
    "        avg_width_train = width_train.mean(0)[0]\n",
    "    \n",
    "        ind_train = (y_train >= li_train) & (y_train <= ui_train)\n",
    "        coverage_train= ind_train.mean(0)[0]\n",
    "    \n",
    "        averaged_targets_train = G_u_train.mean(0).reshape(-1,1)\n",
    "        rmse_train = np.sqrt(((y_train -averaged_targets_train)**2).mean(0))[0]\n",
    "        \n",
    "        pearsonr_train = pearsonr(averaged_targets_train.reshape(averaged_targets_train.shape[0],), \n",
    "                                 y_train.reshape(y_train.shape[0],))\n",
    "        \n",
    "        r_train = pearsonr_train.statistic\n",
    "    \n",
    "        G_u_test, _, _, _, _ = get_predictions_test(data1_test, data2_test, data3_test, data4_test, initial_ensembles)\n",
    "    \n",
    "\n",
    "    \n",
    "        li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)     \n",
    "        ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)   \n",
    "    \n",
    "        width_test = ui_test - li_test\n",
    "        avg_width_test = width_test.mean(0)[0]\n",
    "    \n",
    "        ind_test = (y_test >= li_test) & (y_test <= ui_test)\n",
    "        coverage_test= ind_test.mean(0)[0]\n",
    "    \n",
    "        averaged_targets_test = G_u_test.mean(0).reshape(-1,1)\n",
    "        rmse_test = np.sqrt(((y_test -averaged_targets_test)**2).mean(0))[0]  \n",
    "        \n",
    "        pearsonr_test = pearsonr(averaged_targets_test.reshape(averaged_targets_test.shape[0],), \n",
    "                                 y_test.reshape(y_test.shape[0],))\n",
    "        \n",
    "        r_test = pearsonr_test.statistic\n",
    "\n",
    "        # print(\"Training Coverage, Widths, RMSE, and Pearson R\")\n",
    "        # print(coverage_train, avg_width_train, rmse_train, r_train)\n",
    "        # print(\"Testing Coverage, Widths, RMSE, and Pearson R\")\n",
    "        # print(coverage_test, avg_width_test, rmse_test, r_test)\n",
    "        # print(w1.mean(), w1.std())\n",
    "\n",
    "        if (rmse_train < best_rmse_train): \n",
    "            best_rmse_train = rmse_train\n",
    "            # print(\"went here\")\n",
    "            best_train_width_mean = avg_width_train.mean()\n",
    "            best_train_width = avg_width_train\n",
    "            # best_smiles_weight = w1.mean()\n",
    "            best_coverage_train = coverage_train\n",
    "            best_rmse_train = rmse_train\n",
    "            best_pearson_r = r_test\n",
    "            best_test_width = avg_width_test\n",
    "\n",
    "            best_coverage_test = coverage_test    \n",
    "            best_rmse_test = rmse_test\n",
    "            patience = 0\n",
    "            best_ensembles = initial_ensembles\n",
    "            \n",
    "        else:\n",
    "            patience = patience + 1\n",
    "            \n",
    "        # print(\"Patience is\")\n",
    "        # print(patience)\n",
    "        # print('\\n')\n",
    "        \n",
    "        if patience > threshold:\n",
    "            \n",
    "            # print()\n",
    "            # print(best_train_width.tolist(), best_coverage_train.tolist(), best_rmse_train.tolist(), best_test_width.tolist(), best_coverage_test.tolist(), best_rmse_test.tolist(), best_smiles_weight, flush = True)\n",
    "            # print(\"done for fold\" + str(idx), flush = True)\n",
    "            print(\"train_width\" + str(best_train_width), flush = True)\n",
    "            print(\"test_width\" + str(best_test_width), flush = True)\n",
    "            print(\"pearson\" + str(best_pearson_r), flush = True)\n",
    "            print(\"rmse_train\" + str(best_rmse_train), flush = True)\n",
    "            print(\"rmse_test\" + str(best_rmse_test), flush = True)\n",
    "            print(\"train_coverage\" + str(coverage_train), flush = True)\n",
    "            print(\"test_coverage\" + str(coverage_test), flush = True)\n",
    "            # print(\"smiles_weight_ci\" + str([best_li_smiles_weight, best_ui_smiles_weight]), flush = True)\n",
    "            \n",
    "            return [best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r]\n",
    "            # return [best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r, best_ensembles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8c623-2952-409f-a640-7d89cef6b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da638a8-a762-4cfa-9d19-11eeecd4a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be012348-9356-4c87-a59c-e77f7f6c95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch = Parallel(n_jobs = 14, backend = \"loky\", verbose = 8)(delayed(get_results)(idx, var_weights = 8.0, var_weight_weights = 8.0, var_D = 1, inflation_factor =1, fudging_beta = beta(1,19), \n",
    "           fudging_var =1*1e-2) for idx in range(0,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977da878-9085-46b6-b74c-1202687e2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data//deepcdr_dualgcn_50_results_from_jupyter.pickle\", \"wb\") as f: \n",
    "    pickle.dump(catch, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f630f1c-7ced-4d70-ade3-43921a90a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = pd.DataFrame(catch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a271002-b697-4c78-ad69-89ef0f17432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.columns = ['best_train_width', 'best_coverage_train', 'best_rmse_train', 'best_test_width', 'best_coverage_test', 'best_rmse_test', 'best_pearson_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82903e54-a510-4d28-b92b-32869c4257d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "10ad5718-39fc-4fb6-808a-97c63f5b79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(idx, var_weights = 1.0, var_weight_weights = 1.0, var_D = 0.01, inflation_factor = 1.6, fudging_beta = beta(1,19), \n",
    "               fudging_var = 1e-3):\n",
    "    \n",
    "    # smiles_feats_train, rdkit_feats_train, smiles_feats_valid, rdkit_feats_valid, y_train, y_train_actual, y_valid, y_valid_actual, initial_ensembles  = prepare_data(idx, var_weights = var_weights, var_weight_weights =var_weight_weights, var_L = var_L, var_D = var_D)\n",
    "    \n",
    "    \n",
    "    data1_train, data2_train, data3_train, data4_train, y_train =  prepare_data_train(catch_train, size = 2000)\n",
    "    \n",
    "    data1_test, data2_test, data3_test, data4_test, y_test =  prepare_data_test(catch_test, size = 1000)\n",
    "    \n",
    "    X_t, initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_D = get_initial_X_t(data1_train, data2_train, data3_train, data4_train,\n",
    "                                                                                                 size_ens = size_ens, var_weights = var_weights,\n",
    "                                                                                                var_weight_weights = var_weight_weights, var_D = var_D)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    best_rmse_train = 100000\n",
    "    \n",
    "    for i in range(0,10000):\n",
    "        print(\"epoch number is \" +str(i))\n",
    "\n",
    "        initial_ensembles = get_updated_ensemble(data1_train, data2_train, data3_train, data4_train, initial_ensembles, y_train, size_ens = size_ens,\n",
    "                                                 inflation_factor = inflation_factor, fudging_beta = fudging_beta, fudging_var = fudging_var)\n",
    "        \n",
    "        G_u_train, w1, w2, w3, w4 = get_predictions(data1_train, data2_train, data3_train, data4_train, initial_ensembles, fudging_beta)\n",
    "    \n",
    "        li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)    \n",
    "        ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)  \n",
    "    \n",
    "        width_train = ui_train - li_train\n",
    "        avg_width_train = width_train.mean(0)[0]\n",
    "    \n",
    "        ind_train = (y_train >= li_train) & (y_train <= ui_train)\n",
    "        coverage_train= ind_train.mean(0)[0]\n",
    "    \n",
    "        averaged_targets_train = G_u_train.mean(0).reshape(-1,1)\n",
    "        rmse_train = np.sqrt(((y_train -averaged_targets_train)**2).mean(0))[0]\n",
    "        \n",
    "        pearsonr_train = pearsonr(averaged_targets_train.reshape(averaged_targets_train.shape[0],), \n",
    "                                 y_train.reshape(y_train.shape[0],))\n",
    "        \n",
    "        r_train = pearsonr_train.statistic\n",
    "    \n",
    "        G_u_test, _, _, _, _ = get_predictions_test(data1_test, data2_test, data3_test, data4_test, initial_ensembles)\n",
    "    \n",
    "\n",
    "    \n",
    "        li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)     \n",
    "        ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)   \n",
    "    \n",
    "        width_test = ui_test - li_test\n",
    "        avg_width_test = width_test.mean(0)[0]\n",
    "    \n",
    "        ind_test = (y_test >= li_test) & (y_test <= ui_test)\n",
    "        coverage_test= ind_test.mean(0)[0]\n",
    "    \n",
    "        averaged_targets_test = G_u_test.mean(0).reshape(-1,1)\n",
    "        rmse_test = np.sqrt(((y_test -averaged_targets_test)**2).mean(0))[0]  \n",
    "        \n",
    "        pearsonr_test = pearsonr(averaged_targets_test.reshape(averaged_targets_test.shape[0],), \n",
    "                                 y_test.reshape(y_test.shape[0],))\n",
    "        \n",
    "        r_test = pearsonr_test.statistic\n",
    "\n",
    "        print(\"Training Coverage, Widths, RMSE, and Pearson R\")\n",
    "        print(coverage_train, avg_width_train, rmse_train, r_train)\n",
    "        print(\"Testing Coverage, Widths, RMSE, and Pearson R\")\n",
    "        print(coverage_test, avg_width_test, rmse_test, r_test)\n",
    "        # print(w1.mean(), w1.std())\n",
    "\n",
    "        if (rmse_train < best_rmse_train): \n",
    "            best_rmse_train = rmse_train\n",
    "            # print(\"went here\")\n",
    "            best_train_width_mean = avg_width_train.mean()\n",
    "            best_train_width = avg_width_train\n",
    "            # best_smiles_weight = w1.mean()\n",
    "            best_coverage_train = coverage_train\n",
    "            best_rmse_train = rmse_train\n",
    "            best_pearson_r = r_test\n",
    "            best_test_width = avg_width_test\n",
    "\n",
    "            best_coverage_test = coverage_test    \n",
    "            best_rmse_test = rmse_test\n",
    "            patience = 0\n",
    "            best_ensembles = initial_ensembles\n",
    "            \n",
    "        else:\n",
    "            patience = patience + 1\n",
    "            \n",
    "        print(\"Patience is\")\n",
    "        print(patience)\n",
    "        print('\\n')\n",
    "        \n",
    "        if patience > threshold:\n",
    "            \n",
    "            # print()\n",
    "            # print(best_train_width.tolist(), best_coverage_train.tolist(), best_rmse_train.tolist(), best_test_width.tolist(), best_coverage_test.tolist(), best_rmse_test.tolist(), best_smiles_weight, flush = True)\n",
    "            # print(\"done for fold\" + str(idx), flush = True)\n",
    "            print(\"train_width\" + str(best_train_width), flush = True)\n",
    "            print(\"test_width\" + str(best_test_width), flush = True)\n",
    "            print(\"pearson\" + str(best_pearson_r), flush = True)\n",
    "            print(\"rmse_train\" + str(best_rmse_train), flush = True)\n",
    "            print(\"rmse_test\" + str(best_rmse_test), flush = True)\n",
    "            print(\"train_coverage\" + str(coverage_train), flush = True)\n",
    "            print(\"test_coverage\" + str(coverage_test), flush = True)\n",
    "            # print(\"smiles_weight_ci\" + str([best_li_smiles_weight, best_ui_smiles_weight]), flush = True)\n",
    "            \n",
    "            # return [best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r]\n",
    "            return [best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r, best_ensembles]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "14658528-ea1a-4a6f-83bd-f708df0bcd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number is 0\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.9785 173.09273848542438 31.929956618457545 -0.010866592134597751\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.966 175.19619831595807 32.074692093972516 0.007346771729958277\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 1\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.4195 56.20226135524341 48.90934281162753 0.07008944304188422\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.42 56.69847446531372 49.70511122881298 0.08525065521774665\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 2\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.632 39.81285452651653 20.70477121688562 -0.1756434041292263\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.615 40.32541680941054 21.46244506174813 -0.17359212026373033\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 3\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "0.954 42.04600452917693 11.558593683796676 -0.015061425260643163\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "0.928 43.231028437252995 11.893453477071809 0.04533762182741816\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 4\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 43.194105074777426 5.1817887590266505 0.201776800470455\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 44.08080261605668 5.502788754441057 0.18122743873268957\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 5\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 42.51442928239592 3.46080440308743 0.5040367615881671\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 43.601588761783674 3.6302576816199124 0.49284571786693093\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 6\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 44.129773271149055 2.7369151202226942 0.4661065133512806\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 45.00351347496055 2.905560776851597 0.4096745144437321\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 7\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 44.18483104895236 2.237061108047496 0.6576230335172448\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 45.35853672086943 2.35464596550592 0.6394355706869239\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 8\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 46.11257393575506 2.001164007049987 0.7219020898443423\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 46.943163220241125 2.0999834937066653 0.7014561181659605\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 9\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 44.40911303082749 2.048358905092141 0.7046989728356912\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 45.13846756414145 2.2515287343387946 0.6438490585308928\n",
      "Patience is\n",
      "1\n",
      "\n",
      "\n",
      "epoch number is 10\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 46.79779185994341 2.0847501990913737 0.7355643720442299\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 47.552825074591574 2.2102036754714423 0.7236274891854876\n",
      "Patience is\n",
      "2\n",
      "\n",
      "\n",
      "epoch number is 11\n",
      "Training Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 44.68505367037947 1.871685300653509 0.7615473242958022\n",
      "Testing Coverage, Widths, RMSE, and Pearson R\n",
      "1.0 45.62127311635285 1.9321085080931066 0.7508784268259561\n",
      "Patience is\n",
      "0\n",
      "\n",
      "\n",
      "epoch number is 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[72], line 22\u001b[0m, in \u001b[0;36mget_results\u001b[0;34m(idx, var_weights, var_weight_weights, var_D, inflation_factor, fudging_beta, fudging_var)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch number is \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i))\n\u001b[0;32m---> 22\u001b[0m     initial_ensembles \u001b[38;5;241m=\u001b[39m \u001b[43mget_updated_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata2_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata3_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata4_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_ensembles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_ens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize_ens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43minflation_factor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minflation_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfudging_beta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfudging_beta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfudging_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfudging_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     G_u_train, w1, w2, w3, w4 \u001b[38;5;241m=\u001b[39m get_predictions(data1_train, data2_train, data3_train, data4_train, initial_ensembles, fudging_beta)\n\u001b[1;32m     27\u001b[0m     li_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpercentile(G_u_train, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, q \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2.5\u001b[39m, \u001b[38;5;241m97.5\u001b[39m))[\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)    \n",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m, in \u001b[0;36mget_updated_ensemble\u001b[0;34m(data1, data2, data3, data4, initial_ensembles, y_train, size_ens, inflation_factor, fudging_beta, fudging_var)\u001b[0m\n\u001b[1;32m      3\u001b[0m mu_bar, G_bar, G_u \u001b[38;5;241m=\u001b[39m calculate_mu_bar_G_bar(data1, data2, data3, data4, initial_ensembles, fudging_beta)\n\u001b[1;32m      4\u001b[0m C, G_u_minus_G_bar \u001b[38;5;241m=\u001b[39m calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u)\n\u001b[0;32m----> 5\u001b[0m D \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_D_u\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_u\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m _, R_t \u001b[38;5;241m=\u001b[39m create_cov(data1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],initial_ensembles)\n\u001b[1;32m      7\u001b[0m inflation \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39midentity(R_t\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m*\u001b[39minflation_factor\n",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m, in \u001b[0;36mcalculate_D_u\u001b[0;34m(G_bar, G_u)\u001b[0m\n\u001b[1;32m      3\u001b[0m d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((G_bar\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], G_bar\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, size_ens): \n\u001b[0;32m----> 5\u001b[0m     d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkron\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG_u_minus_G_bar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_u_minus_G_bar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d\u001b[38;5;241m/\u001b[39msize_ens\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mkron\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/enkf/lib/python3.11/site-packages/numpy/lib/shape_base.py:1179\u001b[0m, in \u001b[0;36mkron\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1177\u001b[0m b_arr \u001b[38;5;241m=\u001b[39m expand_dims(b_arr, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, nd\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# In case of `mat`, convert result to `array`\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_any_mat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;66;03m# Reshape back\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreshape(_nx\u001b[38;5;241m.\u001b[39mmultiply(as_, bs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r, best_ensembles = get_results(0, var_weights = 1.0, var_weight_weights = 1.0, var_D = 1, inflation_factor =1, fudging_beta = beta(1,19), \n",
    "           fudging_var =5*1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9490af0c-5a1b-43fb-8431-b1282db34b54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_ensembles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds_test, _, _, _, _ \u001b[38;5;241m=\u001b[39m get_predictions_test(data1_test, data2_test, data3_test, data4_test, \u001b[43mbest_ensembles\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_ensembles' is not defined"
     ]
    }
   ],
   "source": [
    "preds_test, _, _, _, _ = get_predictions_test(data1_test, data2_test, data3_test, data4_test, best_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779268c-ab61-4401-80a0-44538877c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_preds_test = preds_test.mean(0).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f093e5-520c-41f0-a675-01ac46f87a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, averaged_preds_test, alpha = 0.10)\n",
    "plt.axline((0,0), slope = 1, c = \"black\")\n",
    "plt.xlabel(\"Ground Truth Values\")\n",
    "plt.ylabel(\"MEnKF-ANN Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6dbeca-651e-4d31-89c0-c44e35b4270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items = [best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c969db3-06fd-4a30-8f3e-1d1670a03aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_df = pd.DataFrame(items).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f44639-4484-45c7-bd7c-7745ab18371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_df.columns = ['best_train_width', 'best_coverage_train', 'best_rmse_train', 'best_test_width', 'best_coverage_test', 'best_rmse_test', 'best_pearson_r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab228b9-c61a-46cb-aa57-7acf4da3b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_df = items_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c745c-7a4d-4d78-8f47-9a1d7e11a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_df.columns = [\"Value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ed649-b0ba-445d-b28a-ffeb603dc960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743cac6-4f17-49ab-9ad0-f35b685d22f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0a569-aed6-42db-afa8-83819760c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = random.sample(range(y_test.shape[0]), k = 16)\n",
    "fig, axs = plt.subplots(8, 2,figsize=(15, 15))\n",
    "axs = axs.ravel()\n",
    "for idx, i in enumerate(random_idx):\n",
    "    # print(counter)\n",
    "    truth = y_test[i,:]\n",
    "    preds = preds_test[:, i]\n",
    "    percts = np.percentile(preds, axis = 0, q = (2.5, 97.5))\n",
    "    lis = percts[0]\n",
    "    uis = percts[1]\n",
    "    \n",
    "    \n",
    "    axs[idx].hist(preds)\n",
    "    axs[idx].axvline(truth, color='green', linewidth=2)\n",
    "    axs[idx].axvline(lis, color='red', linewidth=2)\n",
    "    axs[idx].axvline(uis, color='red', linewidth=2)\n",
    "\n",
    "# plt.title\n",
    "# fig.savefig('gcn_cdr_pred_intervals.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31471813-3497-4224-acf3-2bd150a35690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(y_train)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cbcc74-a96c-45c2-9e18-c15480f2be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(y_test)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enkf",
   "language": "python",
   "name": "enkf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
